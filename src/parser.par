module Parser

import "./lexer" (Loc, Token)

enum Maybe<T> {
  Some(T) @some
  None @none
}

// TODO: remove this once there's a builtin ref type
enum Ref { DontUseThis }

enum Node {
  Module(Loc, Node, [Node], [Node]) @module
  Import(Loc, Node, [Node]) @import
  Variants(Loc, [Char]) @variants

  Enum(Loc, Node, [Node]) @enum
  Option(Loc, Node, [Node], Maybe<Node>) @option
  Struct(Loc, Node, [Node]) @struct
  Interface(Loc, Node, [Node], [Node]) @interface
  Impl(Loc, Ref, Node, Node, [Node]) @impl
  Global(Loc, Node, Node, Bool) @global
  Fn(Loc, Ref, [Node], Node) @fn
  Sig(Loc, Node, Node) @sig
  ExprSig(Loc, Ref, Node, Node) @expr_sig

  Unit(Loc) @unit
  LitInt(Loc, Int) @int
  LitFloat(Loc, Float) @float
  LitBool(Loc, Bool) @bool
  LitChar(Loc, Char) @char
  LitStr(Loc, String) @str
  LitAtom(Loc, Atom) @atom

  Var(Loc, [Char]) @var
  VarRef(Loc, Ref, [Char]) @var_ref
  VarValue(Loc, [Char]) @var_value
  Underscore(Loc) @"_"

  List(Loc, [Node]) @list
  Cons(Loc, [Node], Node) @cons
  Tuple(Loc, [Node]) @tuple
  Map(Loc, [(Node, Node)]) @map

  FieldFn(Loc, Node) @field_fn
  Field(Loc, Node, Node) @field

  AnonRecord(Loc, [Node]) @anon_record
  Record(Loc, Node, [Node]) @record
  Init(Loc, Node, Node) @init

  AnonRecordExt(Loc, Node, [Node]) @anon_record_ext
  RecordExt(Loc, Node, Node, [Node]) @record_ext
  Ext(Loc, Node, Node) @ext

  App(Loc, Node, [Node]) @app
  Native(Loc, Node, Node, Int) @native

  Con(Loc, [Char]) @con_token
  TV(Loc, [Char], [Node]) @tv_te
  Gen(Loc, Node, [Node]) @gen_te
  LamTE(Loc, Node, Node) @lam_te
  TupleTE(Loc, [Node]) @tuple_te
  RecordTE(Loc, [Node]) @record_te
  RecordExtTE(Loc, Node, [Node]) @record_ext_te

  BinaryExpr(Loc, BinaryOpAtom, Node, Node) @binary_op
  UnaryExpr(Loc, UnaryOpAtom, Node) @unary_op

  If(Loc, Node, Node, Node) @if
  Let(Loc, [Node], Node) @let
  Binding(Loc, Node, Node) @binding
  IfLet(Loc, Node, Node, Node, Node) @if_let
  Match(Loc, Node, [Node]) @match
  Case(Loc, Node, Node) @case
  Block(Loc, [Node]) @block
}

enum NodeOp {
  Node(Node)
  BinaryOp(BinaryOpAtom)
  UnaryOp(Loc, UnaryOpAtom)
}

enum BinaryOpAtom {
  Pipe @"|>"
  Or @"||"
  And @"&&"
  Eq @"=="
  NotEq @"!="
  Gt @">"
  GtEq @">="
  Lt @"<"
  LtEq @"<="
  Add @"+"
  Sub @"-"
  Concat @"++"
  Separate @"--"
  Mul @"*"
  Div @"/"
  Mod @"%"
  Colon @":"
  Dot @"."
}

enum UnaryOpAtom {
  Neg @"-"
  Not @"!"
  Pound @"#"
  Dollar @"$"
  Discard @"discard"
}

enum OpAtom {
  Bin(BinaryOpAtom)
  Un(UnaryOpAtom)
}

struct Parsed<T> {
  value : Maybe<T>
  errs : [(Maybe<Loc>, String)]
}

enum State {
  Start
  AcceptOp
}

parse : [Token] -> Parsed<Node>
export parse(tokens) = match tokens {
  [Lexer.Module(s), Lexer.Con(e, con) | t] =>
    imports(t) |> pbind(|(imports, rest)|
      many(def, rest) |> pmap(Module(range(s, e), Con(e, con), imports))
    )

  [Lexer.Module(_) | t] =>
    add_err(
      many(def, tokens),
      "Expected module name to follow module keyword",
      head_loc(t)
    )

  _ =>
    add_err(
      many(def, tokens),
      "Expected 'module' keyword",
      head_loc(tokens)
    )
}

//--- Productions
imports : [Token] -> Parsed<([Node], [Token])>
imports(tokens) = match tokens {
  [Lexer.Import(s), Lexer.LitStr(e, str) | t] =>
    let
      parsed_import =
        if let [Lexer.LParen(_) | t] = t then
          enclosed_paren(s, t) |> pbind(|(l, inner, rest)|
            many_delim_comma_or_nl(import_ident, inner)
              |> pmap_or(|v| v, [])
              |> pmap(|idents| (Import(l, LitStr(e, str), idents), rest))
          )
        else
          parsed((Import(range(s, e), LitStr(e, str), []), t))
    in
      parsed_import |> pbind(|(node, rest)|
        imports(rest) |> pmap(|(nodes, rest)| ([node | nodes], rest))
      )

  [Lexer.Import(_) | t] =>
    head_loc(t) |> one_err("Expected string path to import")

  _ => parsed(([], tokens))
}

import_ident : [Token] -> Parsed<(Node, [Token])>
import_ident(tokens) = let variants = to_list("variants") in match tokens {
  [Lexer.Var(s, &variants), Lexer.Con(e, name) | t] =>
    parsed((Variants(range(s, e), name), t))
  [Lexer.Var(l, name) | t] => parsed((Var(l, name), t))
  [Lexer.Con(l, name) | t] => parsed((Con(l, name), t))
}

def : [Token] -> Parsed<(Node, [Token])>
def(tokens) = match tokens {
  [Lexer.Enum(l) | t] => enumeration([Lexer.Enum(l) | t])
  [Lexer.Struct(l) | t] => structure([Lexer.Struct(l) | t])
  [Lexer.Interface(l) | t] => iface([Lexer.Interface(l) | t])
  [Lexer.Impl(l) | t] => implementation([Lexer.Impl(l) | t])

  [Lexer.Var(s, name), Lexer.Colon(_) | t] =>
    type_expr(t) |> pmap(|(node, rest)|
      (Sig(range(s, node_loc(node)), Var(s, name), node), rest)
    )

  [Lexer.Var(_, _) | _] =>
    init(tokens) |> pmap(|(node, rest)|
      let Init(l, var, rhs) = node in (Global(l, var, rhs, false), rest)
    )

  [Lexer.Export(s), Lexer.Var(var_l, name) | t] =>
    init([Lexer.Var(var_l, name) | t]) |> pmap(|(node, rest)|
      let Init(l, var, rhs) = node in
        (Global(range(s, l), var, rhs, true), rest)
    )

  _ => head_loc(tokens) |> one_err(
    "Expected either global definition, global signature, keyword 'enum', " ++
    "or keyword 'struct'"
  )
}

init : [Token] -> Parsed<(Node, [Token])>
init = init_or_ext(false)

init_or_ext : Bool -> [Token] -> Parsed<(Node, [Token])>
init_or_ext(allow_ext, tokens) = match tokens {
  [Lexer.Var(s, name), Lexer.LParen(_), Lexer.RParen(_), Lexer.Assign(_) | t] =>
    expr(t) |> pmap(|(node, rest)|
      let l = range(s, node_loc(node)) in
        (Init(l, Var(s, name), Fn(l, make_ref(), [], node)), rest)
    )

  [Lexer.Var(s, name), Lexer.LParen(_), Lexer.RParen(_), Lexer.Ext(ext_l) | t] =>
    if allow_ext then
      expr(t) |> pmap(|(node, rest)|
        let l = range(s, node_loc(node)) in
          (Ext(l, Var(s, name), Fn(l, make_ref(), [], node)), rest)
      )
    else
      Some(ext_l) |> one_err("Expected equals '='")

  [Lexer.Var(_, _), Lexer.LParen(_), Lexer.RParen(_) | t] =>
    if allow_ext then
      head_loc(t) |> one_err("Expected equals '=' or extend ':='")
    else
      head_loc(t) |> one_err("Expected equals '='")

  [Lexer.Var(s, name), Lexer.LParen(_) | t] =>
    parse_args(pattern, s, t) |> pbind(|v| match v {
      (_, args, [Lexer.Ext(ext_l) | t]) =>
        if allow_ext then
          expr(t) |> pmap(|(node, rest)|
            let l = range(s, node_loc(node)) in
              (Ext(l, Var(s, name), Fn(l, make_ref(), args, node)), rest)
          )
        else
          Some(ext_l) |> one_err("Expected equals '='")

      (_, args, [Lexer.Assign(_) | t]) =>
        expr(t) |> pmap(|(node, rest)|
          let l = range(s, node_loc(node)) in
            (Init(l, Var(s, name), Fn(l, make_ref(), args, node)), rest)
        )

      (_, _, t) => head_loc(t) |> one_err("Expected equals '='")
    })

  [Lexer.Var(s, name), Lexer.Assign(_) | t] =>
    expr(t) |> pmap(|(node, rest)|
      let l = range(s, node_loc(node)) in
        (Init(l, Var(s, name), node), rest)
    )

  [Lexer.Var(s, name), Lexer.Ext(ext_l) | t] =>
    if allow_ext then
      expr(t) |> pmap(|(node, rest)|
        let l = range(s, node_loc(node)) in
          (Ext(l, Var(s, name), node), rest)
      )
    else
      Some(ext_l) |> one_err("Expected equals '='")

  [Lexer.Var(_, _) | t] =>
    if allow_ext then
      head_loc(t) |> one_err(
        "Expected '(', '=', or ':=' to follow variable identifier in " ++
        "global definition"
      )
    else
      head_loc(t) |> one_err(
        "Expected '(' or '=' to follow variable identifier in global " ++
        "definition"
      )

  _ => head_loc(tokens) |> one_err("Expected variable identifier")
}

enumeration : [Token] -> Parsed<(Node, [Token])>
enumeration(tokens) = match tokens {
  [Lexer.Enum(s) | t] =>
    new_type_braces(s, t) |> pbind(|(l, node, inner, rest)|
      many_delim_comma_or_nl(option, inner)
        |> pmap_or(|v| v, [])
        |> pmap(|options| (Enum(l, node, options), rest))
    )

  _ => head_loc(tokens) |> one_err("Expected keyword 'enum'")
}

tv_node : [Token] -> Parsed<(Node, [Token])>
tv_node(tokens) = match tokens {
  [Lexer.TV(l, tv) | t] => parsed((TV(l, tv, []), t))
  _ => head_loc(tokens) |> one_err("Expected type variable")
}

qualified_con : [Token] -> Parsed<(Node, [Token])>
qualified_con(tokens) = match tokens {
  [Lexer.Con(s, mod) | t] => try_qualify_con(s, mod, t) |> parsed
  _ => head_loc(tokens) |> one_err("Expected capitalized name")
}

option : [Token] -> Parsed<(Node, [Token])>
option(tokens) = match tokens {
  [Lexer.Con(s, name), Lexer.LParen(_) | t] =>
    parse_args(type_expr, s, t) |> pmap(|v| match v {
      (l, args, [Lexer.LitAtom(atom_l, atom) | t]) =>
        (Option(
          range(l, atom_l),
          Con(s, name),
          args,
          Some(LitAtom(atom_l, atom))
        ), t)

      (l, args, t) => (Option(l, Con(s, name), args, None), t)
    })

  [Lexer.Con(s, name), Lexer.LitAtom(e, atom) | t] =>
    (Option(range(s, e), Con(s, name), [], Some(LitAtom(e, atom))), t)
      |> parsed

  [Lexer.Con(l, name) | t] =>
    (Option(l, Con(l, name), [], None), t) |> parsed

  _ => head_loc(tokens) |> one_err("Expected capitalized name of option")
}

structure : [Token] -> Parsed<(Node, [Token])>
structure(tokens) = match tokens {
  [Lexer.Struct(s) | t] =>
    new_type_braces(s, t) |> pbind(|(l, node, inner, rest)|
      many_delim_comma_or_nl(field_sig, inner)
        |> pmap_or(|v| v, [])
        |> pmap(|field_sigs| (Struct(l, node, field_sigs), rest))
    )

  _ => head_loc(tokens) |> one_err("Expected keyword 'struct'")
}

new_type_braces : Loc -> [Token] -> Parsed<(Loc, Node, [Token], [Token])>
new_type_braces(s, tokens) = match tokens {
  [Lexer.Con(con_l, con), Lexer.Lt(_) | t] =>
    enclosed_tag(con_l, t) |> pbind(|v| match v {
      (gen_l, inner, [Lexer.LBrace(_) | t]) =>
        many_delim_comma(tv_node, inner)
          |> pmap_or(|v| v, [])
          |> pbind(|tvs| (enclosed_brace(s, t) |> pmap(|v| (v, tvs))))
          |> pmap(|((l, inner, rest), tvs)|
            (l, Gen(gen_l, Con(con_l, con), tvs), inner, rest)
          )

      (_, _, t) => head_loc(t) |> one_err("Expected open brace '{'")
    })

  [Lexer.Con(con_l, con), Lexer.LBrace(_) | t] =>
    enclosed_brace(s, t) |> pmap(|(l, inner, rest)|
      (l, Con(con_l, con), inner, rest)
    )

  [Lexer.Con(_, _) | t] =>
    head_loc(t) |> one_err("Expected open brace '{' or less than '<'")

  _ => head_loc(tokens) |> one_err("Expected capitalized type name")
}

iface(tokens) = match tokens {
  [Lexer.Interface(s), Lexer.Con(con_l, name) | t] =>
    let
      extends = to_list("extends")
      parsed_parents =
        if let [Lexer.Var(_, &extends) | t] = t then
          many_delim_cb(|node, rest| match rest {
            [Lexer.LBrace(_) | _] => parsed((false, rest))
            [_ | _] => delim_comma_cb(node, rest)
            [] => head_loc(rest) |> one_err("Expected open brace '{'")
          }, qualified_con, t)
        else
          parsed(([], t))
    in
      parsed_parents |> pbind(|(parents, rest)| match rest {
        [Lexer.LBrace(brace_s) | t] =>
          enclosed_brace(brace_s, t) |> pbind(|(brace_l, inner, rest)|
            many_delim_comma_or_nl(field_sig, inner)
              |> pmap_or(|v| v, [])
              |> pmap(|field_sigs|
                let l = range(s, brace_l), con_node = Con(con_l, name) in
                  ((Interface(l, con_node, parents, field_sigs), rest))
              )
          )

        _ => head_loc(rest) |> one_err("Expected open brace '{'")
      })
}

implementation(tokens) = let for = to_list("for") in match tokens {
  [Lexer.Impl(s), Lexer.Con(con_s, mod) | t] =>
    let (con_node, t) = try_qualify_con(con_s, mod, t) in match t {
      [Lexer.Var(_, &for) | t] =>
        type_expr(t) |> pbind(|(te, rest)|
          if let TV(l, _, _) = te then
            Some(l) |> one_err("Expected concrete type, not type variable")
          else match rest {
            [Lexer.LBrace(brace_l) | t] =>
              enclosed_brace(brace_l, t) |> pbind(|(l, inner, rest)|
                many(init, inner)
                  |> pmap_or(|v| v, [])
                  |> pmap(|globals|
                    (Impl(range(s, l), make_ref(), con_node, te, globals), rest)
                  )
              )

            // TODO: should we have an enclosed_brace that accepts open brace
            // token as well and reports this error?
            _ => head_loc(rest) |> one_err("Expected open brace '{'")
          }
        )

      _ => head_loc(t) |> one_err("Expected keyword 'for'")
    }

  _ => head_loc(tokens) |> one_err("Expected keyword 'impl'")
}

expr : [Token] -> Parsed<(Node, [Token])>
expr(tokens) = expr_r(Start, [], tokens)

expr_r : State -> [NodeOp] -> [Token] -> Parsed<(Node, [Token])>
expr_r(state, stack, tokens) = match state {
  Start =>
    let
      recurse(t, node) = expr_r(AcceptOp, [Node(node) | stack], t)
    in match tokens {
      [Lexer.LitInt(l, int) | t] => LitInt(l, int) |> recurse(t)
      [Lexer.LitFloat(l, float) | t] => LitFloat(l, float) |> recurse(t)
      [Lexer.LitBool(l, bool) | t] => LitBool(l, bool) |> recurse(t)
      [Lexer.LitChar(l, char) | t] => LitChar(l, char) |> recurse(t)
      [Lexer.LitStr(l, str) | t] => LitStr(l, str) |> recurse(t)

      [Lexer.LitAtom(s, atom), Lexer.Colon(_), Lexer.Var(var_l, name) | t] =>
        match t {
          [Lexer.Div(_), Lexer.LitInt(e, arity) | t] =>
            Native(range(s, e), LitAtom(s, atom), Var(var_l, name), arity)
              |> recurse(t)
          [Lexer.Div(_) | t] =>
            head_loc(t) |> one_err("Expected integer arity")

          [Lexer.LParen(_) | _] =>
            // arity will be updated after parsing args below
            let var = Var(var_l, name) in
              Native(range(s, var_l), LitAtom(s, atom), var, -1) |> recurse(t)

          _ =>
            head_loc(t) |> one_err(
              "Native functions need to be annotated with their arity " ++
              "(e.g. @lists:filter/2) or called immediately so the arity " ++
              "is implied"
            )
        }
      [Lexer.LitAtom(l, atom) | t] => LitAtom(l, atom) |> recurse(t)

      [Lexer.Var(l, name) | t] => VarRef(l, make_ref(), name) |> recurse(t)

      [Lexer.LBracket(s), Lexer.RBracket(e) | t] =>
        List(range(s, e), []) |> recurse(t)
      [Lexer.LBracket(s) | t] =>
        parse_list(expr, s, t) |> pbind(|(node, rest)| recurse(rest, node))

      [Lexer.LParen(s), Lexer.RParen(e) | t] => Unit(range(s, e)) |> recurse(t)
      [Lexer.LParen(s) | t] =>
        parse_tuple(expr, Tuple, s, t)
          |> pbind(|(node, rest)| recurse(rest, node))

      [Lexer.Con(s, mod) | t] =>
        let
          (con_node, field_node, after_con) =
            if let [Lexer.Dot(_), Lexer.Con(e, con) | t] = t then
              let
                con_node = Con(range(s, e), mod ++ ['.' | con])
                field_node = Field(range(s, e), Con(s, mod), Con(e, con))
              in
                (con_node, field_node, t)
            else
              (Con(s, mod), Con(s, mod), t)
        in
          if let [Lexer.LBrace(_) | t] = after_con then
            enclosed_brace(s, t) |> pbind(|(l, inner, rest)|
              // the locations of LBrace and RBrace don't matter here
              expr([Lexer.LBrace(s) | inner] ++ [Lexer.RBrace(s)])
                |> pbind_or(|(node, [])| match node {
                  AnonRecord(_, fields) =>
                    Record(l, con_node, fields) |> recurse(rest)

                  AnonRecordExt(_, base, fields) =>
                    RecordExt(l, con_node, base, fields) |> recurse(rest)

                  // could potentially parse a valid map, but this is actually
                  // the match body
                  _ => field_node |> recurse(after_con)

                // might not parse an expr if it's a match body
                }, field_node |> recurse(after_con))
            )
          else
            field_node |> recurse(after_con)

      [Lexer.Dot(s), Lexer.Var(e, name) | t] =>
        FieldFn(range(s, e), Var(e, name)) |> recurse(t)
      [Lexer.Dot(_) | t] =>
        head_loc(t) |> one_err("Expected variable identifier after dot '.'")

      [Lexer.LBrace(s), Lexer.RBrace(e) | t] =>
        Map(range(s, e), []) |> recurse(t)

      [Lexer.LBrace(s) | t] =>
        enclosed_brace(s, t) |> pbind(|(l, inner, rest)|
          let
            parse_anon_record(_) =
              many_delim_comma_or_nl(init, inner)
                |> pmap_or(|v| v, [])
                |> pmap(|fields| AnonRecord(l, fields))
                |> pbind(recurse(rest))

            parsed_expr =
              expr(inner) |> pbind(|v| match v {
                (_, [Lexer.FatArrow(_) | _]) =>
                  many_delim_comma(kv_pair, inner)
                    |> pmap_or(|pairs| Map(l, pairs), Map(l, []))
                    |> pbind(recurse(rest))

                (_, [Lexer.Semi(_) | _]) =>
                  many_delim(delim_cb(Lexer.Semi, "semicolon ';'"), expr, inner)
                    |> pmap_or(|nodes| Block(l, nodes), Block(l, []))
                    |> pbind(recurse(rest))

                (base, [Lexer.Bar(_) | t]) =>
                  many_delim_comma_or_nl(init_or_ext(true), t)
                    |> pmap_or(|v| v, [])
                    |> pmap(|fields| AnonRecordExt(l, base, fields))
                    |> pbind(recurse(rest))

                (node, []) => Block(l, [node]) |> recurse(rest)

                (_, t) => head_loc(t) |> one_err(
                  "Expected either a fat arrow '=>' to define a key/value " ++
                  "pair in a map literal, a semicolon ';' to separate a " ++
                  "sequence of expressions in block, or a pipe '|' to " ++
                  "update a record"
                )
              })
          in
            init(inner) |> pbind_or(parse_anon_record, parsed_expr)
        )

      [Lexer.Bar(s), Lexer.Sub(_), Lexer.Bar(_) | t] =>
        // lam is the lowest precedence, so it's fine to just recurse here
        expr(t) |> pbind(|(body, rest)|
          Fn(range(s, node_loc(body)), make_ref(), [], body) |> recurse(rest)
        )
      [Lexer.Bar(s) | t] =>
        let
          parsed_args = many_delim_cb(|node, rest| match rest {
            [Lexer.Bar(_) | t] => parsed((false, t))
            [_ | _] => delim_comma_cb(node, rest)
            [] => head_loc(rest) |> one_err("Expected closing pipe '|'")
          }, pattern, t)
        in
          parsed_args |> pbind(|(args, rest)|
            // lam is the lowest precedence, so it's fine to just recurse here
            expr(rest) |> pbind(|(body, rest)|
              Fn(range(s, node_loc(body)), make_ref(), args, body)
                |> recurse(rest)
            )
          )

      [Lexer.Sub(l) | t] => expr_r(Start, [UnaryOp(l, Neg) | stack], t)
      [Lexer.Not(l) | t] => expr_r(Start, [UnaryOp(l, Not) | stack], t)
      [Lexer.Pound(l) | t] => expr_r(Start, [UnaryOp(l, Pound) | stack], t)
      [Lexer.Dollar(l) | t] => expr_r(Start, [UnaryOp(l, Dollar) | stack], t)
      [Lexer.Discard(l) | t] => expr_r(Start, [UnaryOp(l, Discard) | stack], t)

      [Lexer.If(l) | t] =>
        if_expr([Lexer.If(l) | t]) |> pbind(|(node, rest)| recurse(rest, node))

      [Lexer.Let(l) | t] =>
        let_expr([Lexer.Let(l) | t])
          |> pbind(|(node, rest)| recurse(rest, node))

      [Lexer.Match(s) | t] =>
        expr(t) |> pbind(|v| match v {
          (node, [Lexer.LBrace(_) | t]) =>
            enclosed_brace(s, t) |> pbind(|(l, inner, rest)|
              many_delim_comma_or_nl(case, inner)
                |> pmap_or(|v| v, [])
                |> pmap(|cases| Match(l, node, cases))
                |> pbind(recurse(rest))
            )

          (_, t) => head_loc(t) |> one_err("Expected open brace '{'")
        })

      _ => head_loc(tokens) |> one_err("Expected an expression")
    }

  AcceptOp =>
    let
      shunt(bopa, t) =
        let op = BinaryOp(bopa), next_prec = get(Bin(bopa), opa_prec) in
          expr_r(Start, [op | reduce(next_prec, stack)], t)

      head_end_line(stack) =
        let Node(node) = head(stack) in node_loc(node).end_line

      finish() =
        let [Node(node)] = reduce(0, stack) in parsed((node, tokens))
    in match tokens {
      [Lexer.Pipe(_) | t] => shunt(Pipe, t)
      [Lexer.Or(_) | t] => shunt(Or, t)
      [Lexer.And(_) | t] => shunt(And, t)
      [Lexer.Eq(_) | t] => shunt(Eq, t)
      [Lexer.NotEq(_) | t] => shunt(NotEq, t)
      [Lexer.Gt(_) | t] => shunt(Gt, t)
      [Lexer.GtEq(_) | t] => shunt(GtEq, t)
      [Lexer.Lt(_) | t] => shunt(Lt, t)
      [Lexer.LtEq(_) | t] => shunt(LtEq, t)
      [Lexer.Add(_) | t] => shunt(Add, t)
      [Lexer.Sub(_) | t] => shunt(Sub, t)
      [Lexer.Concat(_) | t] => shunt(Concat, t)
      [Lexer.Separate(_) | t] => shunt(Separate, t)
      [Lexer.Mul(_) | t] => shunt(Mul, t)
      [Lexer.Div(_) | t] => shunt(Div, t)
      [Lexer.Mod(_) | t] => shunt(Mod, t)

      [Lexer.Colon(_) | t] =>
        type_expr(t) |> pbind(|(node, rest)|
          let op = BinaryOp(Colon), next_prec = get(Bin(Colon), opa_prec) in
            expr_r(AcceptOp, [Node(node), op | reduce(next_prec, stack)], rest)
        )

      // TODO: flatten app?
      [Lexer.LParen(s), Lexer.RParen(e) | t] =>
        if s.start_line > head_end_line(stack) then
          finish()
        else
          let
            // field access and app have equal precedence above all else
            [Node(n) | left] = reduce(get(Bin(Dot), opa_prec), stack)

            // set native arity if not given
            node =
              if let Native(l, atom, var, arity) = n then
                if arity == -1 then
                  Native(l, atom, var, 0)
                else
                  n
              else
                n

            app = App(range(node_loc(node), e), node, [Unit(range(s, e))])
          in
            expr_r(AcceptOp, [Node(app) | left], t)
      [Lexer.LParen(s) | t] =>
        if s.start_line > head_end_line(stack) then
          finish()
        else
          parse_args(expr, s, t) |> pbind(|(l, args, rest)|
            let
              // field access and app have equal precedence above all else
              [Node(n) | left] = reduce(get(Bin(Dot), opa_prec), stack)

              // set native arity if not given
              node =
                if let Native(l, atom, var, arity) = n then
                  if arity == -1 then
                    Native(l, atom, var, length(args))
                  else
                    n
                else
                  n

              app = App(range(node_loc(node), l), node, args)
            in
              expr_r(AcceptOp, [Node(app) | left], rest)
          )

      [Lexer.Dot(_), Lexer.Var(e, name) | t] =>
        let
          // field access and app have equal precedence above all else
          [Node(node) | left] = reduce(get(Bin(Dot), opa_prec), stack)
          prop =
            if let Con(_, _) = node then
              // accessing a variable from a module
              VarRef(e, make_ref(), name)
            else
              // accessing a field from a record
              Var(e, name)
          field = Field(range(node_loc(node), e), node, prop)
        in
          expr_r(AcceptOp, [Node(field) | left], t)
      [Lexer.Dot(_) | t] =>
        head_loc(t) |> one_err("Expected variable identifier after dot '.'")

      _ => finish()
    }
}

kv_pair : [Token] -> Parsed<((Node, Node), [Token])>
kv_pair(tokens) = expr(tokens) |> pbind(|v| match v {
  (key, [Lexer.FatArrow(_) | t]) =>
    expr(t) |> pmap(|(value, rest)| ((key, value), rest))

  (_, t) => head_loc(t) |> one_err("Expected fat arrow '=>'")
})

type_expr : [Token] -> Parsed<(Node, [Token])>
type_expr(tokens) = type_expr_r(None, tokens)

type_expr_r : Maybe<Node> -> [Token] -> Parsed<(Node, [Token])>
type_expr_r(lhs, tokens) = match lhs {
  None =>
    let recurse(t, node) = type_expr_r(Some(node), t) in match tokens {
      [Lexer.Con(s, mod) | t] =>
        let (node, t) = try_qualify_con(s, mod, t) in
          type_params(node_loc(node), t) |> pbind(|(l, params, rest)|
            if length(params) == 0 then
              recurse(rest, node)
            else
              recurse(rest, Gen(l, node, params))
          )

      [Lexer.TV(tv_l, tv) | t] =>
        type_params(tv_l, t) |> pbind(|(params_l, params, rest)|
          ifaces(params_l, rest) |> pbind(|(l, ifaces, rest)|
            if length(params) == 0 then
              TV(l, tv, ifaces) |> recurse(rest)
            else
              Gen(l, TV(l, tv, ifaces), params) |> recurse(rest)
          )
        )

      [Lexer.LBracket(s) | t] =>
        enclosed_bracket(s, t) |> pbind(|(l, inner, rest)|
          type_expr(inner)
            |> pbind(|(param, rest)|
              if length(rest) > 0 then
                head_loc(rest) |> one_err("Expected closing ']'")
              else
                parsed([param])
            )
            |> pmap_or(|v| v, [])
            |> pmap(|params| Gen(l, Con(l, to_list("List")), params))
            |> pbind(recurse(rest))
        )

      [Lexer.LParen(s), Lexer.RParen(e) | t] => Unit(range(s, e)) |> recurse(t)
      [Lexer.LParen(s) | t] =>
        parse_tuple(type_expr, TupleTE, s, t)
          |> pbind(|(node, rest)| recurse(rest, node))

      [Lexer.LBrace(s), Lexer.TV(tv_l, tv), Lexer.Bar(_) | t] =>
        enclosed_brace(s, t) |> pbind(|(l, inner, rest)|
          many_delim_comma_or_nl(field_sig, inner)
            |> pmap_or(|v| v, [])
            |> pmap(|field_sigs|
              RecordExtTE(l, TV(tv_l, tv, []), field_sigs)
            )
            |> pbind(recurse(rest))
        )
      [Lexer.LBrace(s) | t] =>
        enclosed_brace(s, t) |> pbind(|(l, inner, rest)|
          many_delim_comma_or_nl(field_sig, inner)
            |> pmap_or(|v| v, [])
            |> pmap(|field_sigs| RecordTE(l, field_sigs))
            |> pbind(recurse(rest))
        )

      _ => head_loc(tokens) |> one_err("Expected a type expression")
    }

  Some(left) => match tokens {
    [Lexer.Arrow(_) | t] =>
      type_expr(t) |> pmap(|(right, rest)|
        (LamTE(range(node_loc(left), node_loc(right)), left, right), rest)
      )

    _ => parsed((left, tokens))
  }
}

type_params : Loc -> [Token] -> Parsed<(Loc, [Node], [Token])>
type_params(l, tokens) = match tokens {
  [Lexer.Lt(s) | t] =>
    enclosed_tag(s, t) |> pbind(|(params_l, inner, rest)|
      many_delim_comma(type_expr, inner)
        |> pmap_or(|v| v, [])
        |> pmap(|params| (range(l, params_l), params, rest))
    )

  _ => parsed((l, [], tokens))
}

ifaces : Loc -> [Token] -> Parsed<(Loc, [Node], [Token])>
ifaces(l, tokens) = ifaces_r(l, tokens, [])

ifaces_r : Loc -> [Token] -> [Node] -> Parsed<(Loc, [Node], [Token])>
ifaces_r(l, tokens, ifaces) = match tokens {
  [Lexer.Tilde(_), Lexer.Con(con_s, mod) | t] =>
    let
      (con_node, t) = try_qualify_con(con_s, mod, t)
      l = range(l, node_loc(con_node))
    in ifaces_r(l, t, [con_node | ifaces])

  [Lexer.Tilde(_) | t] =>
    head_loc(t) |> one_err("Expected capitalized name of interface")

  _ => parsed((l, reverse(ifaces), tokens))
}

field_sig : [Token] -> Parsed<(Node, [Token])>
field_sig(tokens) = match tokens {
  [Lexer.Var(s, name), Lexer.Colon(_) | t] =>
    type_expr(t) |> pmap(|(node, rest)|
      (Sig(range(s, node_loc(node)), Var(s, name), node), rest)
    )

  [Lexer.Var(_, _) | t] =>
    head_loc(t) |> one_err("Expected start of type signature ':'")

  _ => head_loc(tokens) |> one_err("Expected field name")
}

if_expr : [Token] -> Parsed<(Node, [Token])>
if_expr(tokens) = match tokens {
  [Lexer.If(s), Lexer.Let(_) | t] => pattern(t) |> pbind(|v| match v {
    (left, [Lexer.Assign(_) | t]) =>
      expr(t) |> pbind(|(right, rest)|
        then_else(rest) |> pmap(|((then_expr, else_expr), rest)|
          let l = range(s, node_loc(else_expr)) in
            (IfLet(l, left, right, then_expr, else_expr), rest)
        )
      )

    (_, t) => head_loc(t) |> one_err("Expected equals '='")
  })

  [Lexer.If(s) | t] => expr(t) |> pbind(|(cond, rest)|
    then_else(rest) |> pmap(|((then_expr, else_expr), rest)|
      (If(range(s, node_loc(else_expr)), cond, then_expr, else_expr), rest)
    )
  )

  _ => head_loc(tokens) |> one_err("Expected keyword 'if'")
}

then_else : [Token] -> Parsed<((Node, Node), [Token])>
then_else(tokens) = match tokens {
  [Lexer.Then(_) | t] => expr(t) |> pbind(|v| match v {
    (then_expr, [Lexer.Else(_) | t]) =>
      expr(t) |> pmap(|(else_expr, rest)| ((then_expr, else_expr), rest))

    (then_expr, t) => ((then_expr, Unit(node_loc(then_expr))), t) |> parsed
  })

  _ => head_loc(tokens) |> one_err("Expected keyword 'then'")
}

let_expr : [Token] -> Parsed<(Node, [Token])>
let_expr(tokens) = match tokens {
  [Lexer.Let(s) | t] =>
    let
      parsed_bindings = many_delim_cb(|node, rest| match rest {
        [Lexer.In(_) | t] => parsed((false, t))
        [_ | _] => delim_comma_or_nl_cb(node, rest)
        [] => head_loc(rest) |> one_err("Expected keyword 'in'")
      }, binding, t)
    in
      parsed_bindings |> pbind(|(bindings, rest)|
        expr(rest) |> pmap(|(body, rest)|
          (Let(range(s, node_loc(body)), bindings, body), rest)
        )
      )

  _ => head_loc(tokens) |> one_err("Expected keyword 'let'")
}

binding : [Token] -> Parsed<(Node, [Token])>
binding(tokens) = match tokens {
  [Lexer.Var(s, name), Lexer.LParen(_), Lexer.RParen(_) | t] =>
    if let [Lexer.Assign(_) | t] = t then
      expr(t) |> pmap(|(body, rest)|
        let e = node_loc(body), lam = Fn(range(s, e), make_ref(), [], body) in
          (Binding(range(s, e), Var(s, name), lam), rest)
      )
    else
      head_loc(t) |> one_err("Expected equals '='")

  [Lexer.Var(s, name), Lexer.LParen(_) | t] =>
    parse_args(pattern, s, t) |> pbind(|v| match v {
      (_, args, [Lexer.Assign(_) | t]) =>
        expr(t) |> pmap(|(body, rest)|
          let
            e = node_loc(body)
            lam = Fn(range(s, e), make_ref(), args, body)
          in (Binding(range(s, e), Var(s, name), lam), rest)
        )

      (_, _, t) => head_loc(t) |> one_err("Expected equals '='")
    })

  _ => pattern(tokens) |> pbind(|v| match v {
    (left, [Lexer.Assign(_) | t]) =>
      expr(t) |> pmap(|(right, rest)|
        (Binding(range(node_loc(left), node_loc(right)), left, right), rest)
      )

    (_, t) => head_loc(t) |> one_err("Expected equals '='")
  })
}

pattern : [Token] -> Parsed<(Node, [Token])>
pattern(tokens) = let finish(t, node) = parsed((node, t)) in match tokens {
  [Lexer.LitInt(l, int) | t] => LitInt(l, int) |> finish(t)
  [Lexer.LitFloat(l, float) | t] => LitFloat(l, float) |> finish(t)
  [Lexer.LitBool(l, bool) | t] => LitBool(l, bool) |> finish(t)
  [Lexer.LitChar(l, char) | t] => LitChar(l, char) |> finish(t)
  [Lexer.LitStr(l, str) | t] => LitStr(l, str) |> finish(t)
  [Lexer.LitAtom(l, atom) | t] => LitAtom(l, atom) |> finish(t)

  [Lexer.Var(l, name) | t] => Var(l, name) |> finish(t)
  [Lexer.Amp(s), Lexer.Var(e, name) | t] =>
    VarValue(range(s, e), name) |> finish(t)
  [Lexer.Amp(_) | t] =>
    head_loc(t) |> one_err("Expected variable identifier after ampersand '&'")
  [Lexer.Underscore(l) | t] => Underscore(l) |> finish(t)

  [Lexer.Con(s, mod), Lexer.Dot(_), Lexer.Con(e, con) | t] =>
    let node = Field(range(s, e), Con(s, mod), Con(e, con)) in
      if let [Lexer.LParen(_) | t] = t then
        parse_args(pattern, s, t) |> pmap(|(l, args, rest)|
          (App(l, node, args), rest)
        )
      else
        node |> finish(t)
  [Lexer.Con(_, _), Lexer.Dot(_) | t] =>
    head_loc(t) |> one_err("Expected capitalized identifier after dot '.'")
  [Lexer.Con(s, con) | t] =>
    let node = Con(s, con) in
      if let [Lexer.LParen(_) | t] = t then
        parse_args(pattern, s, t) |> pmap(|(l, args, rest)|
          (App(l, node, args), rest)
        )
      else
        node |> finish(t)

  [Lexer.LBracket(s), Lexer.RBracket(e) | t] =>
    List(range(s, e), []) |> finish(t)
  [Lexer.LBracket(s) | t] => parse_list(pattern, s, t)

  [Lexer.LParen(s) | t] => parse_tuple(pattern, Tuple, s, t)

  _ => head_loc(tokens) |> one_err("Expected pattern")
}

case : [Token] -> Parsed<(Node, [Token])>
case(tokens) = pattern(tokens) |> pbind(|v| match v {
  (left, [Lexer.FatArrow(_) | t]) =>
    expr(t) |> pmap(|(right, rest)|
      (Case(range(node_loc(left), node_loc(right)), left, right), rest)
    )

  (_, t) => head_loc(t) |> one_err("Expected fat arrow '=>' after pattern")
})


//--- Helpers
reduce : Int -> [NodeOp] -> [NodeOp]
reduce(next_prec, stack) =
  match last_op(stack) {
    Some(BinaryOp(bopa)) =>
      if next_prec <= get(Bin(bopa), opa_prec) then
        let
          [Node(right), BinaryOp(&bopa), Node(left) | t] = stack
          l = range(node_loc(left), node_loc(right))
          node =
            if bopa == Colon then
              ExprSig(l, make_ref(), left, right)
            else
              BinaryExpr(l, bopa, left, right)
        in reduce(next_prec, [Node(node) | t])
      else
        stack

    Some(UnaryOp(s, uopa)) =>
      if next_prec <= get(Un(uopa), opa_prec) then
        let
          [Node(right), UnaryOp(&s, &uopa) | t] = stack
          l = range(s, node_loc(right))
        in
          reduce(next_prec, [Node(UnaryExpr(l, uopa, right)) | t])
      else
        stack

    None => stack
  }

last_op : [NodeOp] -> Maybe<NodeOp>
last_op(stack) = match stack {
  [BinaryOp(opa) | _] => Some(BinaryOp(opa))
  [UnaryOp(l, opa) | _] => Some(UnaryOp(l, opa))
  [_ | t] => last_op(t)
  [] => None
}

opa_prec : Map<OpAtom, Int>
opa_prec =
  let pairs = map_index(|opas, i| map(|opa| (opa, i), opas), opa_prec_list) in
    @maps:from_list(@lists:append(pairs))

opa_prec_list : [[OpAtom]]
opa_prec_list = [
  [Bin(Pipe)],
  [Bin(Or)],
  [Bin(And)],
  [Bin(Eq), Bin(NotEq), Bin(Gt), Bin(GtEq), Bin(Lt), Bin(LtEq)],
  [Bin(Add), Bin(Sub), Bin(Concat), Bin(Separate)],
  [Bin(Mul), Bin(Div), Bin(Mod)],
  [Bin(Colon)],
  [Un(Neg), Un(Not), Un(Pound), Un(Dollar), Un(Discard)],
  [Bin(Dot)]
]

try_qualify_con : Loc -> [Char] -> [Token] -> (Node, [Token])
try_qualify_con(s, mod, tokens) =
  if let [Lexer.Dot(_), Lexer.Con(e, con) | t] = tokens then
    (Con(range(s, e), mod ++ ['.' | con]), t)
  else
    (Con(s, mod), tokens)

parse_list : ([Token] -> Parsed<(Node, [Token])>) -> Loc -> [Token] ->
  Parsed<(Node, [Token])>
parse_list(f, s, tokens) =
  enclosed_bracket(s, tokens) |> pbind(|(l, inner, rest)|
    split_bar(inner)
      |> pbind(|(left, right)|
        many_delim_comma(f, left) |> pmap(|elems| (elems, right))
      )
      |> pbind(|(elems, right)|
        if length(right) == 0 then
          parsed(List(l, elems))
        else
          f(right) |> pbind(|(base, rest)|
            if length(rest) > 0 then
              head_loc(rest) |> one_err("Expected closing ']'")
            else
              parsed(Cons(l, elems, base))
          )
      )
      |> pmap_or(|v| (v, rest), (List(l, []), rest))
  )

parse_tuple : ([Token] -> Parsed<(Node, [Token])>) ->
  (Loc -> [Node] -> Node) -> Loc -> [Token] -> Parsed<(Node, [Token])>
parse_tuple(f, wrap, s, tokens) =
  enclosed_paren(s, tokens) |> pbind(|(l, inner, rest)|
    f(inner) |> pbind(|(node, t)| match t {
      [Lexer.Comma(_)] =>
        parsed_err(
          (wrap(l, []), rest),
          "Expected at least two elements within tuple",
          Some(l)
        )

      [Lexer.Comma(_) | t] =>
        many_delim_comma(f, t)
          |> pmap_or(|elems| wrap(l, [node | elems]), wrap(l, []))
          |> pmap(|tuple| (tuple, rest))

      [h | _] =>
        parsed_err(
          (wrap(l, []), rest),
          "Expected comma ',' delimiter or closing paren ')'",
          Some(token_loc(h))
        )

      [] => parsed((@erlang:setelement(2, node, l), rest))
    })
  )

parse_args : ([Token] -> Parsed<(Node, [Token])>) -> Loc -> [Token] ->
  Parsed<(Loc, [Node], [Token])>
parse_args(f, s, tokens) =
  enclosed_paren(s, tokens) |> pbind(|(l, inner, rest)|
    many_delim_comma(f, inner)
      |> pmap_or(|v| v, [])
      |> pmap(|args| (l, args, rest))
  )

many : ([Token] -> Parsed<(T, [Token])>) -> [Token] -> Parsed<[T]>
many(f, tokens) = let p = f(tokens) in match p.value {
  Some((v, [])) => { p | value := Some([v]) }
  Some((v, t)) => let new_r = many(f, t) in match new_r.value {
    Some(vs) => { new_r | value = Some([v | vs]), errs = new_r.errs ++ p.errs }
    None => { new_r | errs = new_r.errs ++ p.errs }
  }

  None => { p | value := None }
}

many_delim_cb : (T -> [Token] -> Parsed<(Bool, [Token])>) ->
  ([Token] -> Parsed<(T, [Token])>) -> [Token] -> Parsed<([T], [Token])>
many_delim_cb(cb, f, tokens) = f(tokens) |> pbind(|(item, rest)|
  cb(item, rest) |> pbind(|(continue, rest)|
    if !continue then
      parsed(([item], rest))
    else if rest == [] then
      one_err("many_delim didn't finish", None)
    else
      many_delim_cb(cb, f, rest) |> pmap(|(items, rest)| ([item | items], rest))
  )
)

delim_cb : (Loc -> Token) -> String -> T -> [Token] -> Parsed<(Bool, [Token])>
delim_cb(delim, delim_str, _, tokens) =
  if let [] = tokens then
    parsed((false, []))
  else
    let [h | t] = tokens, l = token_loc(h) in
      if h == delim(l) then
        parsed((t != [], t))
      else
        head_loc(tokens) |> one_err(
          "Expected " ++ delim_str ++ " delimiter after expression"
        )

delim_comma_cb = delim_cb(Lexer.Comma, "comma ','")

many_delim : (T -> [Token] -> Parsed<(Bool, [Token])>) ->
  ([Token] -> Parsed<(T, [Token])>) -> [Token] -> Parsed<[T]>
many_delim(cb, f, tokens) =
  many_delim_cb(cb, f, tokens) |> pmap(|(items, [])| items)

many_delim_comma : ([Token] -> Parsed<(T, [Token])>) -> [Token] -> Parsed<[T]>
many_delim_comma = many_delim(delim_comma_cb)

delim_or_nl_cb : (Loc -> Token) -> String -> Node -> [Token] ->
  Parsed<(Bool, [Token])>
delim_or_nl_cb(delim, delim_str, node, tokens) =
  if let [] = tokens then
    parsed((false, []))
  else
    let
      [h | t] = tokens
      l = token_loc(h)
      end_line = node_loc(node).end_line
      is_delim = h == delim(l)
    in
      if is_delim && t == [] then
        parsed_err(
          (t != [], t),
          "Delimiting " ++ delim_str ++ " is unnecessary because nothing " ++
            "follows it. Note that if you separate successive items by " ++
            "newlines, no commas are necessary at all",
          Some(l)
        )
      else if is_delim && end_line < token_loc(head(t)).start_line then
        parsed_err(
          (t != [], t),
          "Delimiting " ++ delim_str ++ " is unnecessary because items are " ++
            "separated by a newline",
          Some(l)
        )
      else if is_delim then
        parsed((t != [], t))
      else if end_line < l.start_line then
        parsed((tokens != [], tokens))
      else
        one_err(
          "Expected either " ++ delim_str ++ " or newline delimiter",
          Some(l)
        )

delim_comma_or_nl_cb : Node -> [Token] -> Parsed<(Bool, [Token])>
delim_comma_or_nl_cb = delim_or_nl_cb(Lexer.Comma, "comma ','")

many_delim_comma_or_nl : ([Token] -> Parsed<(Node, [Token])>) -> [Token] ->
  Parsed<[Node]>
many_delim_comma_or_nl(f, tokens) =
  many_delim_cb(delim_comma_or_nl_cb, f, tokens) |> pmap(|(items, [])| items)

enclosed : (Loc -> Token) -> (Loc -> Token) -> String -> Loc -> [Token] ->
  Parsed<(Loc, [Token], [Token])>
enclosed = enclosed_r([], 1)

enclosed_r : [Token] -> Int -> (Loc -> Token) -> (Loc -> Token) -> String ->
  Loc -> [Token] -> Parsed<(Loc, [Token], [Token])>
enclosed_r(inner, depth, open, close, close_str, s, tokens) =
  match tokens {
    [h | t] =>
      let l = token_loc(h) in
        // By checking close first, we allow the caller to set open = close
        // without us recursing forever. Instead, setting open = close means
        // just find the nearest close token, ignoring any nesting.
        if h == close(l) then
          if depth == 1 then
            parsed((range(s, l), reverse(inner), t))
          else
            enclosed_r([h | inner], depth - 1, open, close, close_str, s, t)
        else if h == open(l) then
          enclosed_r([h | inner], depth + 1, open, close, close_str, s, t)
        else
          enclosed_r([h | inner], depth, open, close, close_str, s, t)

    [] => Some(s) |> one_err("Couldn't find matching " ++ close_str)
  }

enclosed_bracket : Loc -> [Token] -> Parsed<(Loc, [Token], [Token])>
enclosed_bracket =
  enclosed(Lexer.LBracket, Lexer.RBracket, "closing bracket ']'")

enclosed_paren : Loc -> [Token] -> Parsed<(Loc, [Token], [Token])>
enclosed_paren = enclosed(Lexer.LParen, Lexer.RParen, "closing paren ')'")

enclosed_brace : Loc -> [Token] -> Parsed<(Loc, [Token], [Token])>
enclosed_brace = enclosed(Lexer.LBrace, Lexer.RBrace, "closing brace '}'")

enclosed_tag : Loc -> [Token] -> Parsed<(Loc, [Token], [Token])>
enclosed_tag = enclosed(Lexer.Lt, Lexer.Gt, "closing greater than '>'")

split_bar : [Token] -> Parsed<([Token], [Token])>
split_bar(tokens) = split_bar_r(tokens, [], false, [], 1)

split_bar_r : [Token] -> [Token] -> Bool -> [Token] -> Int ->
  Parsed<([Token], [Token])>
split_bar_r(tokens, left, seen_bar, right, depth) =
  let
    recurse(token, seen_bar, depth) =
      if seen_bar then
        split_bar_r(tail(tokens), left, seen_bar, [token | right], depth)
      else
        split_bar_r(tail(tokens), [token | left], seen_bar, right, depth)
  in match tokens {
    [Lexer.LBracket(l) | _] => recurse(Lexer.LBracket(l), seen_bar, depth + 1)
    [Lexer.RBracket(l) | _] => recurse(Lexer.RBracket(l), seen_bar, depth - 1)
    [Lexer.Bar(l) | t] =>
      if depth == 1 then
        if seen_bar then
          one_err(
            "Expected an expression; already saw a pipe '|' within this " ++
              "list literal",
            Some(l)
          )
        else
          // shouldn't add a token to left or right, so don't use recurse
          split_bar_r(t, left, true, right, depth)
      else
        recurse(Lexer.Bar(l), seen_bar, depth)

    [h | _] => recurse(h, seen_bar, depth)
    [] => parsed((reverse(left), reverse(right)))
  }

head_loc : [Token] -> Maybe<Loc>
head_loc(tokens) =
  if let [t | _] = tokens then
    Some(@erlang:element(2, t))
  else
    None

node_loc : Node -> Loc
node_loc(node) = @erlang:element(2, node)

token_loc : Token -> Loc
token_loc(token) = @erlang:element(2, token)

range : Loc -> Loc -> Loc
range(start, end) = Loc {
  start_line = start.start_line
  start_col = start.start_col
  end_line = end.end_line
  end_col = end.end_col
}

pmap : (A -> B) -> Parsed<A> -> Parsed<B>
pmap(f, p) =
  let new_value = match p.value {
    Some(a) => Some(f(a))
    None => None
  } in { p | value := new_value }

pmap_or : (A -> B) -> B -> Parsed<A> -> Parsed<B>
pmap_or(f, default, p) =
  let new_value = match p.value {
    Some(a) => Some(f(a))
    None => Some(default)
  } in { p | value := new_value }

pbind : (A -> Parsed<B>) -> Parsed<A> -> Parsed<B>
pbind(f, p) = match p.value {
  Some(a) => let new_r = f(a) in { new_r | errs = new_r.errs ++ p.errs }
  None => { p | value := None }
}

pbind_or : (A -> Parsed<B>) -> Parsed<B> -> Parsed<A> -> Parsed<B>
pbind_or(f, default, p) = match p.value {
  Some(_) => pbind(f, p)
  None => default
}

parsed : A -> Parsed<A>
parsed(a) = Parsed { value = Some(a), errs = [] }

parsed_err : A -> String -> Maybe<Loc> -> Parsed<A>
parsed_err(a, str, mloc) = Parsed { value = Some(a), errs = [(mloc, str)] }

add_err : Parsed<A> -> String -> Maybe<Loc> -> Parsed<B>
add_err(p, str, mloc) = { p | value := None, errs = [(mloc, str) | p.errs] }

one_err : String -> Maybe<Loc> -> Parsed<A>
one_err(str, mloc) = Parsed { value = None, errs = [(mloc, str)] }


//--- "Standard library" functions
get : K -> Map<K, V> -> V
get = @maps:get/2

map_index : (A -> Int -> B) -> [A] -> [B]
map_index(f, l) =
  let (mapped, _) = @lists:mapfoldl(|a, i| (f(a, i), i + 1), 0, l) in mapped

map : (A -> B) -> [A] -> [B]
map = @lists:map/2

head : [A] -> A
head = @erlang:hd/1

tail : [A] -> [A]
tail = @erlang:tl/1

reverse : [A] -> [A]
reverse = @lists:reverse/1

length : [A] -> Int
length = @erlang:length/1

to_list : String -> [Char]
to_list = @erlang:binary_to_list/1

make_ref : () -> Ref
make_ref = @erlang:make_ref/0

print : String -> A -> A
print(prefix, a) = { @io:format(prefix ++ ": ~p~n", [a]); a }
