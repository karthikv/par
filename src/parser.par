module Parser

import "./lexer" (Loc, Token)

enum Node {
  Module(Loc, Node, [Node], [Node]) @module
  Import(Loc, Node, [Node]) @import
  All(Loc) @all
  Variants(Loc, [Char]) @variants

  Enum(Loc, Node, [Node]) @enum
  Option(Loc, Node, [Node], Option<Node>) @option
  Exception(Loc, Node, [Node]) @exception
  Struct(Loc, Node, [Node]) @struct
  Interface(Loc, Node, [Node], [Node]) @interface
  Impl(Loc, Ref, Node, Node, [Node]) @impl
  Global(Loc, Node, Node, Bool) @global
  Fn(Loc, Ref, [Node], Node) @fn
  Sig(Loc, Node, Node) @sig
  ExprSig(Loc, Ref, Node, Node) @expr_sig

  Unit(Loc) @unit
  LitInt(Loc, Int) @int
  LitFloat(Loc, Float) @float
  LitBool(Loc, Bool) @bool
  LitChar(Loc, Char) @char
  LitStr(Loc, String) @str
  LitAtom(Loc, Atom) @atom

  Var(Loc, [Char]) @var
  VarRef(Loc, Ref, [Char]) @var_ref
  VarValue(Loc, [Char]) @var_value
  Hole(Loc) @"hole"
  Underscore(Loc) @"_"

  List(Loc, [Node]) @list
  Cons(Loc, [Node], Node) @cons
  Tuple(Loc, [Node]) @tuple
  Map(Loc, [Node]) @map
  Assoc(Loc, Node, Node) @assoc

  FieldFn(Loc, Node) @field_fn
  Field(Loc, Node, Node) @field

  AnonRecord(Loc, Ref, [Node]) @anon_record
  Record(Loc, Node, [Node]) @record
  Init(Loc, Node, Node) @init

  AnonRecordExt(Loc, Ref, Node, [Node]) @anon_record_ext
  RecordExt(Loc, Node, Node, [Node]) @record_ext
  Ext(Loc, Node, Node) @ext

  App(Loc, Node, [Node]) @app
  Variant(Loc, Node, [Node]) @variant
  Native(Loc, Node, Node, Int) @native

  Con(Loc, [Char]) @con_token
  TV(Loc, [Char], [Node]) @tv_te
  Gen(Loc, Node, [Node]) @gen_te
  LamTE(Loc, [Node], Node) @lam_te
  TupleTE(Loc, [Node]) @tuple_te
  ArgsTE(Loc, [Node]) @args_te
  RecordTE(Loc, [Node]) @record_te
  RecordExtTE(Loc, Node, [Node]) @record_ext_te

  BinaryExpr(Loc, BinaryOpAtom, Node, Node) @binary_op
  UnaryExpr(Loc, UnaryOpAtom, Node) @unary_op

  If(Loc, Node, Node, Node) @if
  Let(Loc, [Node], Node) @let
  Binding(Loc, Node, Node) @binding
  IfLet(Loc, Node, Node, Node, Node) @if_let
  Match(Loc, Node, [Node]) @match
  Try(Loc, Node, [Node]) @try
  Case(Loc, Node, Node) @case
  Ensure(Loc, Node, Node) @ensure
  Block(Loc, [Node]) @block
}

enum NodeOp {
  Node(Node)
  BinaryOp(BinaryOpAtom)
  UnaryOp(Loc, UnaryOpAtom)
}

enum BinaryOpAtom {
  Pipe @"|>"
  Or @"||"
  And @"&&"
  Eq @"=="
  NotEq @"!="
  Gt @">"
  GtEq @">="
  Lt @"<"
  LtEq @"<="
  Add @"+"
  Sub @"-"
  Concat @"++"
  Separate @"--"
  Mul @"*"
  Div @"/"
  Mod @"%"
  Colon @":"
  Dot @"."
}

enum UnaryOpAtom {
  Neg @"-"
  Not @"!"
  Pound @"#"
  Dollar @"$"
  Raise @raise
  Discard @discard
  Assume @assume

  Test @test
  Assert @assert
}

enum OpAtom {
  Bin(BinaryOpAtom)
  Un(UnaryOpAtom)
}

struct Parsed<T> {
  value : Option<T>
  errs : [(Option<Loc>, String)]
}

enum State {
  Start
  AcceptOp
}

enum Transition {
  Push([Token], Node)
  PushOp([Token], NodeOp)
  PushParsed(Parsed<(Node, [Token])>)
}

parse : [Token] -> Parsed<Node>
export parse(tokens) = match tokens {
  [Lexer.Module(s), Lexer.Con(e, con) | t] =>
    imports(t) |> pbind(|(imports, rest)|
      many(def, rest) |> pmap(Module(range(s, e), Con(e, con), imports, _))
    )

  [Lexer.Module(_) | t] =>
    head_loc(t) |> one_err(
      "Expected capitalized module name to follow 'module' keyword"
    )

  _ =>
    let parsed = imports(tokens) |> pbind(|(_, rest)| many(def, rest))
    add_err(
      parsed,
      "Expected 'module' keyword at start of file",
      head_loc(tokens)
    )
}

//--- Productions
imports : [Token] -> Parsed<([Node], [Token])>
imports(tokens) = match tokens {
  [Lexer.Import(s) | t] =>
    let from_parsed =
      if let [Lexer.LitStr(l, str) | t] = t then
        parsed((LitStr(l, str), t))
      else if let [Lexer.Con(l, con) | t] = t then
        parsed((Con(l, con), t))
      else
        head_loc(t) |> one_err(
          "Expected either a string path to a local file or a capitalized " ++
          "standard library module name to import"
        )

    let parsed_import = from_parsed |> pbind(|(from, rest)|
      if let [Lexer.LParen(_) | t] = rest then
        enclosed_paren(s, t) |> pbind(|(l, inner, rest)|
          if let [Lexer.Mul(all_l)] = inner then
            parsed((Import(l, from, [All(all_l)]), rest))
          else
            many_delim_comma_or_nl(import_ident, inner)
            |> pmap_or(|v| v, [])
            |> pmap(|idents| (Import(l, from, idents), rest))
        )
      else
        let l = range(s, node_loc(from))
        parsed((Import(l, from, []), rest))
    )

    parsed_import |> pbind(|(node, rest)|
      imports(rest) |> pmap(|(nodes, rest)| ([node | nodes], rest))
    )

  _ => parsed(([], tokens))
}

import_ident : [Token] -> Parsed<(Node, [Token])>
import_ident(tokens) =
  match tokens {
    [Lexer.Var(s, ['v', 'a', 'r', 'i', 'a', 'n', 't', 's'])
     Lexer.Con(e, name) | t] =>
      parsed((Variants(range(s, e), name), t))
    [Lexer.Var(l, name) | t] => parsed((Var(l, name), t))
    [Lexer.Con(l, name) | t] => parsed((Con(l, name), t))

    _ =>
      let addendum =
        if let [Lexer.Mul(_) | _] = tokens then
          ". If you want to import all names unqualified, the import list " ++
          "must only contain *. For instance, import \"foo\" (*)"
        else
          ""

      head_loc(tokens) |> one_err(
        "Expected an idenitifer to import or the keyword 'variants'" ++
        addendum
      )
  }

def : [Token] -> Parsed<(Node, [Token])>
def(tokens) = match tokens {
  [Lexer.Enum(s) | t] => enumeration(s, t)
  [Lexer.Exception(s) | t] => exc(s, t)
  [Lexer.Struct(s) | t] => structure(s, t)
  [Lexer.Interface(s) | t] => iface(s, t)
  [Lexer.Impl(s) | t] => implementation(s, t)

  [Lexer.Var(s, name), Lexer.Colon(_) | t] =>
    type_expr(t) |> pmap(|(node, rest)|
      (Sig(range(s, node_loc(node)), Var(s, name), node), rest)
    )

  [Lexer.Var(_, _) | _] =>
    init(tokens) |> pmap(|(node, rest)|
      let Init(l, var, rhs) = node
      (Global(l, var, rhs, false), rest)
    )

  [Lexer.Export(s), Lexer.Var(var_l, name) | t] =>
    init([Lexer.Var(var_l, name) | t]) |> pmap(|(node, rest)|
      let Init(l, var, rhs) = node
      (Global(range(s, l), var, rhs, true), rest)
    )

  _ => head_loc(tokens) |> one_err(
    "Expected either global definition, global signature, keyword 'enum', " ++
    "or keyword 'struct'"
  )
}

init : [Token] -> Parsed<(Node, [Token])>
init = init_or_ext(false, _)

init_or_ext : (Bool, [Token]) -> Parsed<(Node, [Token])>
init_or_ext(allow_ext?, tokens) = match tokens {
  [Lexer.Var(s, name), Lexer.LParen(_), Lexer.RParen(_), Lexer.Assign(_) | t] =>
    expr(t) |> pmap(|(node, rest)|
      let l = range(s, node_loc(node))
      (Init(l, Var(s, name), Fn(l, make_ref(), [], node)), rest)
    )

  [Lexer.Var(s, name), Lexer.LParen(_), Lexer.RParen(_), Lexer.Ext(ext_l) | t] =>
    if allow_ext? then
      expr(t) |> pmap(|(node, rest)|
        let l = range(s, node_loc(node))
        (Ext(l, Var(s, name), Fn(l, make_ref(), [], node)), rest)
      )
    else
      Some(ext_l) |> one_err("Expected equals '='")

  [Lexer.Var(_, _), Lexer.LParen(_), Lexer.RParen(_) | t] =>
    if allow_ext? then
      head_loc(t) |> one_err("Expected equals '=' or extend ':='")
    else
      head_loc(t) |> one_err("Expected equals '='")

  [Lexer.Var(s, name), Lexer.LParen(_) | t] =>
    parse_args(pattern, s, t) |> pbind(|v| match v {
      (_, args, [Lexer.Ext(ext_l) | t]) =>
        if allow_ext? then
          expr(t) |> pmap(|(node, rest)|
            let l = range(s, node_loc(node))
            (Ext(l, Var(s, name), Fn(l, make_ref(), args, node)), rest)
          )
        else
          Some(ext_l) |> one_err("Expected equals '='")

      (_, args, [Lexer.Assign(_) | t]) =>
        expr(t) |> pmap(|(node, rest)|
          let l = range(s, node_loc(node))
          (Init(l, Var(s, name), Fn(l, make_ref(), args, node)), rest)
        )

      (_, _, t) => head_loc(t) |> one_err("Expected equals '='")
    })

  [Lexer.Var(s, name), Lexer.Assign(_) | t] =>
    expr(t) |> pmap(|(node, rest)|
      let l = range(s, node_loc(node))
      (Init(l, Var(s, name), node), rest)
    )

  [Lexer.Var(s, name), Lexer.Ext(ext_l) | t] =>
    if allow_ext? then
      expr(t) |> pmap(|(node, rest)|
        let l = range(s, node_loc(node))
        (Ext(l, Var(s, name), node), rest)
      )
    else
      Some(ext_l) |> one_err("Expected equals '='")

  [Lexer.Var(_, _) | t] =>
    if allow_ext? then
      head_loc(t) |> one_err(
        "Expected '(', '=', or ':=' to follow variable identifier in " ++
        "global definition"
      )
    else
      head_loc(t) |> one_err(
        "Expected '(' or '=' to follow variable identifier in global " ++
        "definition"
      )

  _ => head_loc(tokens) |> one_err("Expected variable identifier")
}

enumeration : (Loc, [Token]) -> Parsed<(Node, [Token])>
enumeration(s, tokens) =
  new_type_braces(s, tokens) |> pbind(|(l, node, inner, rest)|
    many_delim_comma_or_nl(option, inner)
    |> pmap_or(|v| v, [])
    |> pmap(|options| (Enum(l, node, options), rest))
  )

option : [Token] -> Parsed<(Node, [Token])>
option(tokens) = match tokens {
  [Lexer.Con(s, name), Lexer.LParen(_) | t] =>
    parse_args(type_expr, s, t) |> pmap(|v| match v {
      (l, args, [Lexer.LitAtom(atom_l, atom) | t]) =>
        (Option(
          range(l, atom_l),
          Con(s, name),
          args,
          Some(LitAtom(atom_l, atom))
        ), t)

      (l, args, t) => (Option(l, Con(s, name), args, None), t)
    })

  [Lexer.Con(s, name), Lexer.LitAtom(e, atom) | t] =>
    (Option(range(s, e), Con(s, name), [], Some(LitAtom(e, atom))), t)
    |> parsed

  [Lexer.Con(l, name) | t] =>
    (Option(l, Con(l, name), [], None), t) |> parsed

  _ => head_loc(tokens) |> one_err("Expected capitalized name of option")
}

exc : (Loc, [Token]) -> Parsed<(Node, [Token])>
exc(s, tokens) = match tokens {
  [Lexer.Con(con_s, name), Lexer.LParen(paren_s) | t] =>
    parse_args(type_expr, paren_s, t) |> pmap(|(args_l, args, rest)|
      (Exception(range(s, args_l), Con(con_s, name), args), rest)
    )

  [Lexer.Con(l, name) | t] =>
    (Exception(range(s, l), Con(l, name), []), t) |> parsed

  _ => head_loc(tokens) |> one_err("Expected capitalized name of exception")
}

structure : (Loc, [Token]) -> Parsed<(Node, [Token])>
structure(s, tokens) =
  new_type_braces(s, tokens) |> pbind(|(l, node, inner, rest)|
    many_delim_comma_or_nl(field_sig, inner)
    |> pmap_or(|v| v, [])
    |> pmap(|field_sigs| (Struct(l, node, field_sigs), rest))
  )

new_type_braces : (Loc, [Token]) -> Parsed<(Loc, Node, [Token], [Token])>
new_type_braces(s, tokens) = match tokens {
  [Lexer.Con(con_l, con), Lexer.Lt(_) | t] =>
    enclosed_tag(con_l, t) |> pbind(|v| match v {
      (gen_l, inner, [Lexer.LBrace(_) | t]) =>
        many_delim_comma(tv_node, inner)
        |> pmap_or(|v| v, [])
        |> pbind(|tvs| (enclosed_brace(s, t) |> pmap(|v| (v, tvs))))
        |> pmap(|((l, inner, rest), tvs)|
          (l, Gen(gen_l, Con(con_l, con), tvs), inner, rest)
        )

      (_, _, t) => head_loc(t) |> one_err("Expected open brace '{'")
    })

  [Lexer.Con(con_l, con), Lexer.LBrace(_) | t] =>
    enclosed_brace(s, t) |> pmap(|(l, inner, rest)|
      (l, Con(con_l, con), inner, rest)
    )

  [Lexer.Con(_, _) | t] =>
    head_loc(t) |> one_err("Expected open brace '{' or less than '<'")

  [Lexer.TV(l, _) | _] =>
    Some(l) |> one_err(
      "Expected capitalized type name, which should be at least two letters " ++
      "long. A single, capitalized letter is a type variable"
    )

  _ => head_loc(tokens) |> one_err("Expected capitalized type name")
}

tv_node : [Token] -> Parsed<(Node, [Token])>
tv_node(tokens) = match tokens {
  [Lexer.TV(l, tv) | t] => parsed((TV(l, tv, []), t))
  _ => head_loc(tokens) |> one_err("Expected type variable")
}

iface : (Loc, [Token]) -> Parsed<(Node, [Token])>
iface(s, tokens) = match tokens {
  [Lexer.Con(con_l, name) | t] =>
    let parsed_parents =
      if let [Lexer.Var(_, ['e', 'x', 't', 'e', 'n', 'd', 's']) | t] = t then
        many_delim_cb(|node, rest| match rest {
          [Lexer.LBrace(_) | _] => parsed((false, rest))
          [_ | _] => delim_comma_cb(node, rest)
          [] => head_loc(rest) |> one_err("Expected open brace '{'")
        }, qualified_con, t)
      else
        parsed(([], t))

    parsed_parents |> pbind(|(parents, rest)| match rest {
      [Lexer.LBrace(brace_s) | t] =>
        enclosed_brace(brace_s, t) |> pbind(|(brace_l, inner, rest)|
          many_delim_comma_or_nl(field_sig, inner)
          |> pmap_or(|v| v, [])
          |> pmap(|field_sigs|
            let l = range(s, brace_l)
            let con_node = Con(con_l, name)
            ((Interface(l, con_node, parents, field_sigs), rest))
          )
        )

      _ => head_loc(rest) |> one_err("Expected open brace '{'")
    })

  _ => head_loc(tokens) |> one_err("Expected capitalized name of interface")
}


qualified_con : [Token] -> Parsed<(Node, [Token])>
qualified_con(tokens) = match tokens {
  [Lexer.Con(s, mod) | t] => try_qualify_con(s, mod, t) |> parsed
  _ => head_loc(tokens) |> one_err("Expected capitalized name")
}

implementation : (Loc, [Token]) -> Parsed<(Node, [Token])>
implementation(s, tokens) =
  match tokens {
    [Lexer.Con(con_s, mod) | t] =>
      let (con_node, t) = try_qualify_con(con_s, mod, t)

      match t {
        [Lexer.Var(_, ['f', 'o', 'r']) | t] =>
          type_expr(t) |> pbind(|(te, rest)|
            if let TV(l, _, _) = te then
              Some(l) |> one_err("Expected concrete type, not type variable")
            else if let Gen(l, TV(_, _, _), _) = te then
              Some(l) |> one_err(
                "Expected concrete type, not a variable generic type"
              )
            else match rest {
              [Lexer.LBrace(brace_l) | t] =>
                enclosed_brace(brace_l, t) |> pbind(|(l, inner, rest)|
                  many(init, inner)
                  |> pmap_or(|v| v, [])
                  |> pmap(|globals|
                    let ref = make_ref()
                    (Impl(range(s, l), ref, con_node, te, globals), rest)
                  )
                )

              // TODO: should we have an enclosed_brace that accepts open brace
              // token as well and reports this error?
              _ => head_loc(rest) |> one_err("Expected open brace '{'")
            }
          )

        _ => head_loc(t) |> one_err("Expected keyword 'for'")
      }

    _ => head_loc(tokens) |> one_err("Expected capitalized interface name")
  }

expr : [Token] -> Parsed<(Node, [Token])>
expr(tokens) = expr_r(head_loc(tokens), Start, [], tokens)

expr_r : (Option<Loc>, State, [NodeOp], [Token]) -> Parsed<(Node, [Token])>
expr_r(expr_s, state, stack, tokens) = match state {
  Start =>
    let transition = match tokens {
      [Lexer.LitInt(l, int) | t] => LitInt(l, int) |> Push(t)
      [Lexer.LitFloat(l, float) | t] => LitFloat(l, float) |> Push(t)
      [Lexer.LitBool(l, bool) | t] => LitBool(l, bool) |> Push(t)
      [Lexer.LitChar(l, char) | t] => LitChar(l, char) |> Push(t)
      [Lexer.LitStr(l, str) | t] => LitStr(l, str) |> Push(t)

      [Lexer.LitAtom(s, atom), Lexer.Colon(_), Lexer.Var(var_l, name) | t] =>
        match t {
          [Lexer.Div(_), Lexer.LitInt(e, arity) | t] =>
            let l = range(s, e)
            let lit_atom = LitAtom(s, atom)
            Native(l, lit_atom, Var(var_l, name), arity) |> Push(t)
          [Lexer.Div(_) | t] =>
            head_loc(t) |> one_err("Expected integer arity") |> PushParsed

          [Lexer.LParen(_) | _] =>
            // arity will be updated after parsing args below
            let var = Var(var_l, name)
            Native(range(s, var_l), LitAtom(s, atom), var, -1) |> Push(t)

          _ =>
            Some(range(s, var_l))
            |> one_err(
              "Native functions need to be annotated with their arity " ++
              "(e.g. @lists:filter/2) or called immediately so the arity " ++
              "is implied"
            )
            |> PushParsed
        }
      [Lexer.LitAtom(l, atom) | t] => LitAtom(l, atom) |> Push(t)

      [Lexer.Var(l, name) | t] => VarRef(l, make_ref(), name) |> Push(t)
      [Lexer.Underscore(l) | t] => Hole(l) |> Push(t)

      [Lexer.LBracket(s), Lexer.RBracket(e) | t] =>
        List(range(s, e), []) |> Push(t)
      [Lexer.LBracket(s) | t] => parse_list(expr, s, t) |> PushParsed

      [Lexer.LParen(s), Lexer.RParen(e) | t] => Unit(range(s, e)) |> Push(t)
      [Lexer.LParen(s) | t] => parse_tuple(expr, Tuple, s, t) |> PushParsed

      [Lexer.Con(s, mod) | t] =>
        let (con_node, field_node, after_con) =
          if let [Lexer.Dot(_), Lexer.Con(e, con) | t] = t then
            let con_node = Con(range(s, e), mod ++ ['.' | con])
            let field_node = Field(range(s, e), Con(s, mod), Con(e, con))
            (con_node, field_node, t)
          else
            (Con(s, mod), Con(s, mod), t)

        if let [Lexer.LBrace(_) | t] = after_con then
          enclosed_brace(s, t)
          |> pbind(|(l, inner, rest)|
            // the locations of LBrace and RBrace don't matter here
            expr([Lexer.LBrace(s) | inner] ++ [Lexer.RBrace(s)])
            |> pmap(|(node, [])| match node {
              AnonRecord(_, _, fields) => (Record(l, con_node, fields), rest)
              AnonRecordExt(_, _, base, fields) =>
                (RecordExt(l, con_node, base, fields), rest)

              // could potentially parse a valid map, but this is actually
              // the match body
              _ => (field_node, after_con)
            })
            // Might parse a pattern, not an expr, if it's a match body. Note
            // that we must use pbind_or instead of pmap_or to remove errors.
            |> pbind_or(|v| parsed(v), parsed((field_node, after_con)))
          )
          |> PushParsed
        else
          field_node |> Push(after_con)

      [Lexer.Dot(s), Lexer.Var(e, name) | t] =>
        FieldFn(range(s, e), Var(e, name)) |> Push(t)
      [Lexer.Dot(_) | t] =>
        head_loc(t)
        |> one_err("Expected variable identifier after dot '.'")
        |> PushParsed

      [Lexer.LBrace(s), Lexer.RBrace(e) | t] =>
        Map(range(s, e), []) |> Push(t)

      [Lexer.LBrace(s) | t] =>
        enclosed_brace(s, t)
        |> pbind(|(l, inner, rest)|
          let parse_anon_record(_) =
            many_delim_comma_or_nl(init, inner)
            |> pmap_or(|v| v, [])
            |> pmap(|fields| AnonRecord(l, make_ref(), fields))

          let parsed_expr = expr(inner) |> pbind(|v| match v {
            (_, [Lexer.FatArrow(_) | _]) =>
              many_delim_comma_or_nl(assoc, inner)
              |> pmap_or(|v| v, [])
              |> pmap(|pairs| Map(l, pairs))

            (base, [Lexer.Bar(_) | t]) =>
              many_delim_comma_or_nl(init_or_ext(true, _), t)
              |> pmap_or(|v| v, [])
              |> pmap(|fields| AnonRecordExt(l, make_ref(), base, fields))

            // If we use head_loc(t) here, we'll incorrectly report an EOF
            // error in the case of { expr }; avoid this by using node_loc
            // instead.
            (node, _) => Some(node_loc(node)) |> one_err(
              "After this expression, expected either a fat arrow '=>' " ++
              "to define a key/value pair in a map literal, or a pipe " ++
              "'|' to update a record"
            )
          })

          init(inner)
          |> pbind_or(parse_anon_record, parsed_expr)
          |> pmap(|v| (v, rest))
        )
        |> PushParsed

      [Lexer.Bar(s), Lexer.Sub(_), Lexer.Bar(_) | t] =>
        // lam, test, and assert have lowest prec, so it's fine to recurse
        expr(t)
        |> pmap(|(body, rest)|
          let l = range(s, node_loc(body))
          (Fn(l, make_ref(), [], body), rest)
        )
        |> PushParsed
      [Lexer.Bar(s) | t] =>
        let parsed_args = many_delim_cb(|node, rest| match rest {
          [Lexer.Bar(_) | t] => parsed((false, t))
          [_ | _] => delim_comma_cb(node, rest)
          [] => head_loc(rest) |> one_err("Expected closing pipe '|'")
        }, pattern, t)

        parsed_args
        |> pbind(|(args, rest)|
          // lam, test, and assert have lowest prec, so it's fine to recurse
          expr(rest) |> pmap(|(body, rest)|
            let l = range(s, node_loc(body))
            (Fn(l, make_ref(), args, body), rest)
          )
        )
        |> PushParsed

      [Lexer.Sub(l) | t] => PushOp(t, UnaryOp(l, Neg))
      [Lexer.Not(l) | t] => PushOp(t, UnaryOp(l, Not))
      [Lexer.Pound(l) | t] => PushOp(t, UnaryOp(l, Pound))
      [Lexer.Dollar(l) | t] => PushOp(t, UnaryOp(l, Dollar))
      [Lexer.Raise(l) | t] => PushOp(t, UnaryOp(l, Raise))
      [Lexer.Discard(l) | t] => PushOp(t, UnaryOp(l, Discard))
      [Lexer.Assume(l) | t] => PushOp(t, UnaryOp(l, Assume))

      // lam, test, and assert have lowest prec, so it's fine to recurse
      [Lexer.Test(s) | t] =>
        expr(t)
        |> pmap(|(node, rest)|
          let l = range(s, node_loc(node))
          (UnaryExpr(l, Test, node), rest)
        )
        |> PushParsed
      [Lexer.Assert(assert_s), Lexer.Let(let_s) | t] =>
        let_expr(assert_s, [Lexer.Let(let_s) | t])
        |> pmap(|(node, rest)|
          let l = range(assert_s, node_loc(node))
          (UnaryExpr(l, Assert, node), rest)
        )
        |> PushParsed
      [Lexer.Assert(s) | t] =>
        expr(t)
        |> pmap(|(node, rest)|
          let l = range(s, node_loc(node))
          (UnaryExpr(l, Assert, node), rest)
        )
        |> PushParsed

      [Lexer.If(s) | t] => if_expr([Lexer.If(s) | t]) |> PushParsed
      [Lexer.Let(s) | t] => let_expr(s, [Lexer.Let(s) | t]) |> PushParsed

      [Lexer.Match(s) | t] =>
        expr_cases(s, t)
        |> pmap(|(l, node, cases, rest)| (Match(l, node, cases), rest))
        |> PushParsed

      [Lexer.Try(s) | t] =>
        expr_cases(s, t)
        |> pmap(|(l, node, cases, rest)| (Try(l, node, cases), rest))
        |> PushParsed

      [Lexer.Ensure(s) | t] =>
        expr(t)
        |> pbind(|v| match v {
          (ensured, [Lexer.After(_) | t]) =>
            expr(t) |> pmap(|(node, rest)|
              let l = node_loc(node) |> range(s)
              (Ensure(l, ensured, node), rest)
            )

          (_, t) => head_loc(t) |> one_err("Expected keyword 'after'")
        })
        |> PushParsed

      _ =>
        head_loc(tokens)
        |> one_err("Expected an expression")
        |> PushParsed
    }

    match transition {
      Push(rest, node) => expr_r(expr_s, AcceptOp, [Node(node) | stack], rest)
      PushOp(rest, op) => expr_r(expr_s, Start, [op | stack], rest)
      PushParsed(p) => p |> pbind(|(node, rest)|
        expr_r(expr_s, AcceptOp, [Node(node) | stack], rest)
      )
    }

  AcceptOp =>
    let shunt(bopa, t) =
      let op = BinaryOp(bopa)
      let next_prec = get(Bin(bopa), opa_prec)
      expr_r(expr_s, Start, [op | reduce(next_prec, stack)], t)

    let head_end_line(stack) =
      let Node(node) = head(stack)
      node_loc(node).end_line

    let finish() =
      let [Node(node)] = reduce(0, stack)
      parsed((node, tokens))

    match tokens {
      [Lexer.Pipe(_) | t] => shunt(Pipe, t)
      [Lexer.Or(_) | t] => shunt(Or, t)
      [Lexer.And(_) | t] => shunt(And, t)
      [Lexer.Eq(_) | t] => shunt(Eq, t)
      [Lexer.NotEq(_) | t] => shunt(NotEq, t)
      [Lexer.Gt(_) | t] => shunt(Gt, t)
      [Lexer.GtEq(_) | t] => shunt(GtEq, t)
      [Lexer.Lt(_) | t] => shunt(Lt, t)
      [Lexer.LtEq(_) | t] => shunt(LtEq, t)
      [Lexer.Add(_) | t] => shunt(Add, t)
      [Lexer.Concat(_) | t] => shunt(Concat, t)
      [Lexer.Separate(_) | t] => shunt(Separate, t)
      [Lexer.Mul(_) | t] => shunt(Mul, t)
      [Lexer.Div(_) | t] => shunt(Div, t)
      [Lexer.Mod(_) | t] => shunt(Mod, t)

      [Lexer.Colon(s), Lexer.Var(_, _) | _] =>
        one_err(
          "A colon can either be used to separate an expression and its " ++
          "corresponding type signature, such as in 1 : Int, or to call a " ++
          "native erlang function, such as in @erlang:length([]). It looks " ++
          "like you're trying to do the latter here, but you must specify " ++
          "a literal atom, like @erlang or @lists, before the colon",
          Some(s)
        )
      [Lexer.Colon(_) | t] =>
        type_expr(t) |> pbind(|(node, rest)|
          let next_prec = get(Bin(Colon), opa_prec)
          let op = BinaryOp(Colon)
          let new_stack = [Node(node), op | reduce(next_prec, stack)]
          expr_r(expr_s, AcceptOp, new_stack, rest)
        )

      [h | _] =>
        let h_l = token_loc(h)
        let aligned? =
          if let Some(s) = expr_s then
            s.start_col == h_l.start_col &&
              h_l.start_line > head_end_line(stack)
          else
            false

        if aligned? then
          expr(tokens) |> pmap(|(second, rest)|
            let [Node(first)] = reduce(0, stack)
            let l = range(node_loc(first), node_loc(second))

            if let Block(_, others) = second then
              (Block(l, [first | others]), rest)
            else
              (Block(l, [first, second]), rest)
          )
        else
          match tokens {
            [Lexer.Sub(_) | t] => shunt(Sub, t)

            [Lexer.LParen(s), Lexer.RParen(e) | t] =>
              if s.start_line > head_end_line(stack) then
                finish()
              else
                // field access and app have equal prec above all else
                let [Node(n) | left] = stack

                // set native arity if not given
                let node =
                  if let Native(l, atom, var, arity) = n then
                    if arity == -1 then
                      Native(l, atom, var, 0)
                    else
                      n
                  else
                    n

                let l = range(node_loc(node), e)
                let app = App(l, node, [])
                expr_r(expr_s, AcceptOp, [Node(app) | left], t)
            [Lexer.LParen(s) | t] =>
              if s.start_line > head_end_line(stack) then
                finish()
              else
                parse_args(expr, s, t) |> pbind(|(l, args, rest)|
                  // field access and app have equal prec above all else
                  let [Node(n) | left] = stack

                  // set native arity if not given
                  let node =
                    if let Native(l, atom, var, arity) = n then
                      if arity == -1 then
                        Native(l, atom, var, length(args))
                      else
                        n
                    else
                      n

                  let app = App(range(node_loc(node), l), node, args)
                  expr_r(expr_s, AcceptOp, [Node(app) | left], rest)
                )

            [Lexer.Dot(_), Lexer.Var(e, name) | t] =>
              // field access and app have equal prec above all else
              let [Node(node) | left] = stack
              let prop =
                if let Con(_, _) = node then
                  // accessing a variable from a module
                  VarRef(e, make_ref(), name)
                else
                  // accessing a field from a record
                  Var(e, name)

              let field = Field(range(node_loc(node), e), node, prop)
              expr_r(expr_s, AcceptOp, [Node(field) | left], t)
            [Lexer.Dot(_) | t] =>
              head_loc(t) |> one_err(
                "Expected variable identifier after dot '.'"
              )

            _ => finish()
          }

      _ => finish()
    }
}

assoc : [Token] -> Parsed<(Node, [Token])>
assoc(tokens) = expr(tokens) |> pbind(|v| match v {
  (key, [Lexer.FatArrow(_) | t]) =>
    expr(t) |> pmap(|(value, rest)|
      let l = range(node_loc(key), node_loc(value))
      (Assoc(l, key, value), rest)
    )

  (_, t) => head_loc(t) |> one_err("Expected fat arrow '=>'")
})

expr_cases : (Loc, [Token]) -> Parsed<(Loc, Node, [Node], [Token])>
expr_cases(s, tokens) =
  expr(tokens) |> pbind(|v| match v {
    (node, [Lexer.LBrace(brace_s) | t]) =>
      enclosed_brace(brace_s, t) |> pbind(|(l, inner, rest)|
        many_delim_comma_or_nl(case, inner)
        |> pmap_or(|v| v, [])
        |> pmap(|cases| (range(s, l), node, cases, rest))
      )

    (_, t) => head_loc(t) |> one_err("Expected open brace '{'")
  })

type_expr : [Token] -> Parsed<(Node, [Token])>
type_expr(tokens) = type_expr_r(None, tokens)

type_expr_r : (Option<Node>, [Token]) -> Parsed<(Node, [Token])>
type_expr_r(lhs, tokens) = match lhs {
  None =>
    let recurse(t, node) = type_expr_r(Some(node), t)
    match tokens {
      [Lexer.Con(s, mod) | t] =>
        let (node, t) = try_qualify_con(s, mod, t)
        type_params(node_loc(node), t) |> pbind(|(l, params, rest)|
          if length(params) == 0 then
            recurse(rest, node)
          else
            recurse(rest, Gen(l, node, params))
        )

      [Lexer.TV(tv_l, tv) | t] =>
        type_params(tv_l, t) |> pbind(|(params_l, params, rest)|
          ifaces(params_l, rest) |> pbind(|(l, ifaces, rest)|
            if length(params) == 0 then
              TV(l, tv, ifaces) |> recurse(rest)
            else
              Gen(l, TV(l, tv, ifaces), params) |> recurse(rest)
          )
        )

      [Lexer.LBracket(s) | t] =>
        enclosed_bracket(s, t) |> pbind(|(l, inner, rest)|
          type_expr(inner)
          |> pbind(|(param, rest)|
            if length(rest) > 0 then
              head_loc(rest) |> one_err("Expected closing ']'")
            else
              parsed([param])
          )
          |> pmap_or(|v| v, [])
          |> pmap(|params| Gen(l, Con(l, ['L', 'i', 's', 't']), params))
          |> pbind(recurse(rest, _))
        )

      [Lexer.LParen(s), Lexer.RParen(e) | t] =>
        Con(range(s, e), ['(', ')']) |> recurse(t)
      [Lexer.LParen(s) | t] =>
        parse_tuple(type_expr, ArgsTE, s, t)
        |> pbind(|(node, rest)| match (node, rest) {
          (ArgsTE(_, _), [Lexer.Arrow(_) | _]) => recurse(rest, node)
          (ArgsTE(l, args), _) => TupleTE(l, args) |> recurse(rest)
          _ => recurse(rest, node)
        })

      [Lexer.LBrace(s), Lexer.TV(tv_l, tv), Lexer.Bar(_) | t] =>
        enclosed_brace(s, t) |> pbind(|(l, inner, rest)|
          many_delim_comma_or_nl(field_sig, inner)
          |> pmap_or(|v| v, [])
          |> pmap(|field_sigs|
            RecordExtTE(l, TV(tv_l, tv, []), field_sigs)
          )
          |> pbind(recurse(rest, _))
        )
      [Lexer.LBrace(s) | t] =>
        enclosed_brace(s, t) |> pbind(|(l, inner, rest)|
          many_delim_comma_or_nl(field_sig, inner)
          |> pmap_or(|v| v, [])
          |> pmap(|field_sigs| RecordTE(l, field_sigs))
          |> pbind(recurse(rest, _))
        )

      _ => head_loc(tokens) |> one_err("Expected a type expression")
    }

  Some(left) => match tokens {
    [Lexer.Arrow(_) | t] =>
      type_expr(t) |> pmap(|(right, rest)|
        let l = range(node_loc(left), node_loc(right))
        let args =
          if let Con(_, ['(', ')']) = left then
            []
          else if let ArgsTE(_, args) = left then
            args
          else
            [left]
        (LamTE(l, args, right), rest)
      )

    _ => parsed((left, tokens))
  }
}

type_params : (Loc, [Token]) -> Parsed<(Loc, [Node], [Token])>
type_params(l, tokens) = match tokens {
  [Lexer.Lt(s) | t] =>
    enclosed_tag(s, t) |> pbind(|(params_l, inner, rest)|
      many_delim_comma(type_expr, inner)
      |> pmap_or(|v| v, [])
      |> pmap(|params| (range(l, params_l), params, rest))
    )

  _ => parsed((l, [], tokens))
}

ifaces : (Loc, [Token]) -> Parsed<(Loc, [Node], [Token])>
ifaces(l, tokens) = ifaces_r(l, tokens, [])

ifaces_r : (Loc, [Token], [Node]) -> Parsed<(Loc, [Node], [Token])>
ifaces_r(l, tokens, ifaces) = match tokens {
  [Lexer.Tilde(_), Lexer.Con(con_s, mod) | t] =>
    let (con_node, t) = try_qualify_con(con_s, mod, t)
    let l = range(l, node_loc(con_node))
    ifaces_r(l, t, [con_node | ifaces])

  [Lexer.Tilde(_) | t] =>
    head_loc(t) |> one_err("Expected capitalized name of interface")

  _ => parsed((l, List.reverse(ifaces), tokens))
}

field_sig : [Token] -> Parsed<(Node, [Token])>
field_sig(tokens) = match tokens {
  [Lexer.Var(s, name), Lexer.Colon(_) | t] =>
    type_expr(t) |> pmap(|(node, rest)|
      (Sig(range(s, node_loc(node)), Var(s, name), node), rest)
    )

  [Lexer.Var(_, _) | t] =>
    head_loc(t) |> one_err("Expected start of type signature ':'")

  _ => head_loc(tokens) |> one_err("Expected field name")
}

if_expr : [Token] -> Parsed<(Node, [Token])>
if_expr(tokens) = match tokens {
  [Lexer.If(s), Lexer.Let(_) | t] => pattern(t) |> pbind(|v| match v {
    (left, [Lexer.Assign(_) | t]) =>
      expr(t) |> pbind(|(right, rest)|
        then_else(rest) |> pmap(|((then_expr, else_expr), rest)|
          let l = range(s, node_loc(else_expr))
          (IfLet(l, left, right, then_expr, else_expr), rest)
        )
      )

    (_, t) => head_loc(t) |> one_err("Expected equals '='")
  })

  [Lexer.If(s) | t] => expr(t) |> pbind(|(cond, rest)|
    then_else(rest) |> pmap(|((then_expr, else_expr), rest)|
      (If(range(s, node_loc(else_expr)), cond, then_expr, else_expr), rest)
    )
  )

  _ => head_loc(tokens) |> one_err("Expected keyword 'if'")
}

then_else : [Token] -> Parsed<((Node, Node), [Token])>
then_else(tokens) = match tokens {
  [Lexer.Then(_) | t] => expr(t) |> pbind(|v| match v {
    (then_expr, [Lexer.Else(_) | t]) =>
      expr(t) |> pmap(|(else_expr, rest)| ((then_expr, else_expr), rest))

    (then_expr, t) => ((then_expr, Unit(node_loc(then_expr))), t) |> parsed
  })

  _ => head_loc(tokens) |> one_err("Expected keyword 'then'")
}

let_expr : (Loc, [Token]) -> Parsed<(Node, [Token])>
let_expr(ref_l, tokens) = match tokens {
  [Lexer.Let(s) | t] =>
    binding(t) |> pbind(|(node, rest)|
      let l = range(s, node_loc(node))
      match rest {
        [h | _] =>
          let h_l = token_loc(h)
          let aligned? = h_l.start_col == ref_l.start_col &&
            h_l.start_line > node_loc(node).end_line

          if aligned? then
            expr(rest) |> pmap(|(body, rest)|
              if let Let(e, bindings, body) = body then
                (Let(range(s, e), [node | bindings], body), rest)
              else
                (Let(range(s, node_loc(body)), [node], body), rest)
            )
          else
            (Let(l, [node], Unit(l)), rest) |> parsed

        [] => (Let(l, [node], Unit(l)), []) |> parsed
      }
    )

  _ => head_loc(tokens) |> one_err("Expected keyword 'let'")
}

binding : [Token] -> Parsed<(Node, [Token])>
binding(tokens) = match tokens {
  [Lexer.Var(s, name), Lexer.LParen(_), Lexer.RParen(_) | t] =>
    if let [Lexer.Assign(_) | t] = t then
      expr(t) |> pmap(|(body, rest)|
        let e = node_loc(body)
        let lam = Fn(range(s, e), make_ref(), [], body)
        (Binding(range(s, e), Var(s, name), lam), rest)
      )
    else
      head_loc(t) |> one_err("Expected equals '='")

  [Lexer.Var(s, name), Lexer.LParen(_) | t] =>
    parse_args(pattern, s, t) |> pbind(|v| match v {
      (_, args, [Lexer.Assign(_) | t]) =>
        expr(t) |> pmap(|(body, rest)|
          let e = node_loc(body)
          let lam = Fn(range(s, e), make_ref(), args, body)
          (Binding(range(s, e), Var(s, name), lam), rest)
        )

      (_, _, t) => head_loc(t) |> one_err("Expected equals '='")
    })

  _ => pattern(tokens) |> pbind(|v| match v {
    (left, [Lexer.Assign(_) | t]) =>
      expr(t) |> pmap(|(right, rest)|
        (Binding(range(node_loc(left), node_loc(right)), left, right), rest)
      )

    (_, t) => head_loc(t) |> one_err("Expected equals '='")
  })
}

pattern : [Token] -> Parsed<(Node, [Token])>
pattern(tokens) =
  let finish(t, node) = parsed((node, t))
  match tokens {
    [Lexer.LitInt(l, int) | t] => LitInt(l, int) |> finish(t)
    [Lexer.Sub(s), Lexer.LitInt(e, int) | t] =>
      LitInt(range(s, e), -int) |> finish(t)

    [Lexer.LitFloat(l, float) | t] => LitFloat(l, float) |> finish(t)
    [Lexer.Sub(s), Lexer.LitFloat(e, float) | t] =>
      LitFloat(range(s, e), -float) |> finish(t)

    [Lexer.LitBool(l, bool) | t] => LitBool(l, bool) |> finish(t)
    [Lexer.LitChar(l, char) | t] => LitChar(l, char) |> finish(t)
    [Lexer.LitStr(l, str) | t] => LitStr(l, str) |> finish(t)
    [Lexer.LitAtom(l, atom) | t] => LitAtom(l, atom) |> finish(t)

    [Lexer.Var(l, name) | t] => Var(l, name) |> finish(t)
    [Lexer.Amp(s), Lexer.Var(e, name) | t] =>
      VarValue(range(s, e), name) |> finish(t)
    [Lexer.Amp(_) | t] =>
      head_loc(t) |> one_err("Expected variable identifier after ampersand '&'")
    [Lexer.Underscore(l) | t] => Underscore(l) |> finish(t)

    [Lexer.Con(s, mod), Lexer.Dot(_), Lexer.Con(e, con) | t] =>
      let node = Field(range(s, e), Con(s, mod), Con(e, con))
      if let [Lexer.LParen(_) | t] = t then
        parse_args(pattern, s, t) |> pmap(|(l, args, rest)|
          (Variant(l, node, args), rest)
        )
      else
        Variant(node_loc(node), node, []) |> finish(t)
    [Lexer.Con(_, _), Lexer.Dot(_) | t] =>
      head_loc(t) |> one_err("Expected capitalized identifier after dot '.'")
    [Lexer.Con(s, con) | t] =>
      let node = Con(s, con)
      if let [Lexer.LParen(_) | t] = t then
        parse_args(pattern, s, t) |> pmap(|(l, args, rest)|
          (Variant(l, node, args), rest)
        )
      else
        Variant(node_loc(node), node, []) |> finish(t)

    [Lexer.LBracket(s), Lexer.RBracket(e) | t] =>
      List(range(s, e), []) |> finish(t)
    [Lexer.LBracket(s) | t] => parse_list(pattern, s, t)

    [Lexer.LParen(s), Lexer.RParen(e) | t] => Unit(range(s, e)) |> finish(t)
    [Lexer.LParen(s) | t] => parse_tuple(pattern, Tuple, s, t)

    _ => head_loc(tokens) |> one_err("Expected pattern")
  }

case : [Token] -> Parsed<(Node, [Token])>
case(tokens) = pattern(tokens) |> pbind(|v| match v {
  (left, [Lexer.FatArrow(_) | t]) =>
    expr(t) |> pmap(|(right, rest)|
      (Case(range(node_loc(left), node_loc(right)), left, right), rest)
    )

  (_, t) => head_loc(t) |> one_err("Expected fat arrow '=>' after pattern")
})


//--- Helpers
reduce : (Int, [NodeOp]) -> [NodeOp]
reduce(next_prec, stack) =
  match last_op(stack) {
    Some(BinaryOp(bopa)) =>
      if next_prec <= get(Bin(bopa), opa_prec) then
        let [Node(right), BinaryOp(&bopa), Node(left) | t] = stack
        let l = range(node_loc(left), node_loc(right))
        let node =
          if bopa == Colon then
            ExprSig(l, make_ref(), left, right)
          else
            BinaryExpr(l, bopa, left, right)

        reduce(next_prec, [Node(node) | t])
      else
        stack

    Some(UnaryOp(s, uopa)) =>
      if next_prec <= get(Un(uopa), opa_prec) then
        let [Node(right), UnaryOp(&s, &uopa) | t] = stack
        let l = range(s, node_loc(right))
        reduce(next_prec, [Node(UnaryExpr(l, uopa, right)) | t])
      else
        stack

    None => stack
  }

last_op : [NodeOp] -> Option<NodeOp>
last_op(stack) = match stack {
  [BinaryOp(opa) | _] => Some(BinaryOp(opa))
  [UnaryOp(l, opa) | _] => Some(UnaryOp(l, opa))
  [_ | t] => last_op(t)
  [] => None
}

opa_prec : Map<OpAtom, Int>
opa_prec =
  List.with_index(opa_prec_list)
  |> List.flat_map(|(opas, i)| map(|opa| (opa, i), opas))
  |> to_map

opa_prec_list : [[OpAtom]]
opa_prec_list = [
  [Bin(Pipe)]
  [Bin(Or)]
  [Bin(And)]
  [Bin(Eq), Bin(NotEq), Bin(Gt), Bin(GtEq), Bin(Lt), Bin(LtEq)]
  [Bin(Add), Bin(Sub), Bin(Concat), Bin(Separate)]
  [Bin(Mul), Bin(Div), Bin(Mod)]
  [Bin(Colon)]
  [Un(Neg), Un(Not), Un(Pound), Un(Dollar), Un(Raise), Un(Discard), Un(Assume)]
]

try_qualify_con : (Loc, [Char], [Token]) -> (Node, [Token])
try_qualify_con(s, mod, tokens) =
  if let [Lexer.Dot(_), Lexer.Con(e, con) | t] = tokens then
    (Con(range(s, e), mod ++ ['.' | con]), t)
  else
    (Con(s, mod), tokens)

parse_list : ([Token] -> Parsed<(Node, [Token])>, Loc, [Token]) ->
  Parsed<(Node, [Token])>
parse_list(f, s, tokens) =
  enclosed_bracket(s, tokens) |> pbind(|(l, inner, rest)|
    many_delim_cb(|node, rest| match rest {
      [Lexer.Bar(_) | _] => parsed((false, rest))
      _ => delim_comma_or_nl_cb(node, rest)
    }, f, inner)
    |> pmap(|(elems, rest)| (flatten_blocks(elems), rest))
    |> pbind(|(elems, rest)|
      let errs = validate_elems(elems)
      if errs == [] then
        parsed((elems, rest))
      else
        Parsed { value = None, errs = errs }
    )
    |> pbind(|(elems, rest)| match rest {
      [Lexer.Bar(_) | t] =>
        f(t) |> pbind(|(base, rest)|
          if empty?(rest) then
            parsed(Cons(l, elems, base))
          else
            head_loc(rest) |> one_err("Expected closing ']'")
        )

      // rest must be empty at this point; delim_comma_or_nl_cb ensures it
      [] => parsed(List(l, elems))
    })
    |> pmap_or(|v| (v, rest), (List(l, []), rest))
  )

flatten_blocks : [Node] -> [Node]
flatten_blocks(elems) = match elems {
  [] => []
  [Block(_, sub_elems) | t] => sub_elems ++ flatten_blocks(t)
  [h | t] => [h | flatten_blocks(t)]
}

validate_elems : [Node] -> [(Option<Loc>, String)]
validate_elems(elems) =
  let let_elems = filter(|node| match node {
    Let(_, _, _) => true
    _ => false
  }, elems)

  let str = "let statements are not allowed as list items"
  map(|Let(s, [binding | _], _)|
    let l = range(s, node_loc(binding))
    (Some(l), str)
  , let_elems)

parse_tuple : (
  [Token] -> Parsed<(Node, [Token])>,
  (Loc, [Node]) -> Node,
  Loc,
  [Token]
) -> Parsed<(Node, [Token])>
parse_tuple(f, wrap, s, tokens) =
  enclosed_paren(s, tokens) |> pbind(|(l, inner, rest)|
    f(inner) |> pbind(|(node, t)| match t {
      [Lexer.Comma(_)] =>
        parsed_err(
          (wrap(l, []), rest),
          "Expected at least two elements within tuple",
          Some(l)
        )

      [Lexer.Comma(_) | t] =>
        many_delim_comma(f, t)
        |> pmap_or(|elems| wrap(l, [node | elems]), wrap(l, []))
        |> pmap(|tuple| (tuple, rest))

      [h | _] =>
        parsed_err(
          (wrap(l, []), rest),
          "Expected comma ',' delimiter or closing paren ')'",
          Some(token_loc(h))
        )

      [] => parsed((@erlang:setelement(2, node, l), rest))
    })
  )

parse_args : ([Token] -> Parsed<(Node, [Token])>, Loc, [Token]) ->
  Parsed<(Loc, [Node], [Token])>
parse_args(f, s, tokens) =
  enclosed_paren(s, tokens) |> pbind(|(l, inner, rest)|
    many_delim_comma(f, inner)
    |> pmap_or(|v| v, [])
    |> pmap(|args| (l, args, rest))
  )

many : ([Token] -> Parsed<(T, [Token])>, [Token]) -> Parsed<[T]>
many(f, tokens) =
  let p = f(tokens)
  match p.value {
    Some((v, [])) => { p | value := Some([v]) }
    Some((v, t)) =>
      let new_r = many(f, t)
      match new_r.value {
        Some(vs) => {
          new_r |
          value = Some([v | vs])
          errs = new_r.errs ++ p.errs
        }
        None => { new_r | errs = new_r.errs ++ p.errs }
      }

    None => { p | value := None }
  }

many_delim_cb : (
  (T, [Token]) -> Parsed<(Bool, [Token])>,
  [Token] -> Parsed<(T, [Token])>,
  [Token]
) -> Parsed<([T], [Token])>
many_delim_cb(cb, f, tokens) = f(tokens) |> pbind(|(item, rest)|
  cb(item, rest) |> pbind(|(continue?, rest)|
    if !continue? then
      parsed(([item], rest))
    else if rest == [] then
      one_err("many_delim didn't finish", None)
    else
      many_delim_cb(cb, f, rest) |> pmap(|(items, rest)| ([item | items], rest))
  )
)

fake_loc : Loc
fake_loc = Loc {
  start_line = -1
  end_line = -1
  start_col = -1
  end_col = -1
}

delim_cb : (Atom, String, T, [Token]) -> Parsed<(Bool, [Token])>
delim_cb(delim_tag, delim_str, _, tokens) =
  if let [] = tokens then
    parsed((false, []))
  else
    let [h | t] = tokens
    if token_tag(h) == delim_tag then
      parsed((t != [], t))
    else
      head_loc(tokens) |> one_err(
        "Expected " ++ delim_str ++ " delimiter after expression"
      )

delim_comma_cb : (T, [Token]) -> Parsed<(Bool, [Token])>
delim_comma_cb =
  let tag = token_tag(Lexer.Comma(fake_loc))
  delim_cb(tag, "comma ','", _, _)

many_delim : (
  (T, [Token]) -> Parsed<(Bool, [Token])>,
  [Token] -> Parsed<(T, [Token])>,
  [Token]
) -> Parsed<[T]>
many_delim(cb, f, tokens) =
  many_delim_cb(cb, f, tokens) |> pmap(|(items, [])| items)

many_delim_comma : ([Token] -> Parsed<(T, [Token])>, [Token]) -> Parsed<[T]>
many_delim_comma = many_delim(delim_comma_cb, _, _)

delim_or_nl_cb : (Atom, String, Node, [Token]) -> Parsed<(Bool, [Token])>
delim_or_nl_cb(delim_tag, delim_str, node, tokens) =
  if let [] = tokens then
    parsed((false, []))
  else
    let [h | t] = tokens
    let l = token_loc(h)
    let delim? = token_tag(h) == delim_tag
    let node_l = node_loc(node)

    if delim? && t == [] then
      parsed((false, t))
    else if delim? then
      let elem_l = token_loc(head(t))
      let aligned? = elem_l.start_col == node_l.start_col &&
        elem_l.start_line > node_l.end_line

      if aligned? then
        Some(l) |> parsed_err(
          (true, t),
          "Delimiting " ++ delim_str ++ " is unnecessary because the next " ++
          "item is aligned and separated by a newline"
        )
      else
        parsed((true, t))
    else if l.start_col == node_l.start_col &&
        l.start_line > node_l.end_line then
      parsed((tokens != [], tokens))
    else
      Some(l) |> one_err(
        "You must separate successive items either with a " ++ delim_str ++
        " or by aligning them on separate lines"
      )

delim_comma_or_nl_cb : (Node, [Token]) -> Parsed<(Bool, [Token])>
delim_comma_or_nl_cb = delim_or_nl_cb(
  token_tag(Lexer.Comma(fake_loc)),
  "comma ','",
  _,
  _,
)

many_delim_comma_or_nl : ([Token] -> Parsed<(Node, [Token])>, [Token]) ->
  Parsed<[Node]>
many_delim_comma_or_nl(f, tokens) =
  many_delim_cb(delim_comma_or_nl_cb, f, tokens) |> pmap(|(items, [])| items)

enclosed : (Atom, Atom, String, Loc, [Token]) -> Parsed<(Loc, [Token], [Token])>
enclosed = enclosed_r([], 1, _, _, _, _, _)

enclosed_r : ([Token], Int, Atom, Atom, String, Loc, [Token]) ->
  Parsed<(Loc, [Token], [Token])>
enclosed_r(inner, depth, open_tag, close_tag, close_str, s, tokens) =
  match tokens {
    [h | t] =>
      // By checking close first, we allow the caller to set open = close
      // without us recursing forever. Instead, setting open = close means
      // just find the nearest close token, ignoring any nesting.
      if token_tag(h) == close_tag then
        if depth == 1 then
          parsed((range(s, token_loc(h)), List.reverse(inner), t))
        else
          enclosed_r([h | inner], depth - 1, open_tag, close_tag, close_str, s, t)
      else if token_tag(h) == open_tag then
        enclosed_r([h | inner], depth + 1, open_tag, close_tag, close_str, s, t)
      else
        enclosed_r([h | inner], depth, open_tag, close_tag, close_str, s, t)

    [] => Some(s) |> one_err("Couldn't find matching " ++ close_str)
  }

enclosed_bracket : (Loc, [Token]) -> Parsed<(Loc, [Token], [Token])>
enclosed_bracket = enclosed(
  token_tag(Lexer.LBracket(fake_loc)),
  token_tag(Lexer.RBracket(fake_loc)),
  "closing bracket ']'",
  _,
  _,
)

enclosed_paren : (Loc, [Token]) -> Parsed<(Loc, [Token], [Token])>
enclosed_paren = enclosed(
  token_tag(Lexer.LParen(fake_loc)),
  token_tag(Lexer.RParen(fake_loc)),
  "closing paren ')'",
  _,
  _,
)

enclosed_brace : (Loc, [Token]) -> Parsed<(Loc, [Token], [Token])>
enclosed_brace = enclosed(
  token_tag(Lexer.LBrace(fake_loc)),
  token_tag(Lexer.RBrace(fake_loc)),
  "closing brace '}'",
  _,
  _,
)

enclosed_tag : (Loc, [Token]) -> Parsed<(Loc, [Token], [Token])>
enclosed_tag = enclosed(
  token_tag(Lexer.Lt(fake_loc)),
  token_tag(Lexer.Gt(fake_loc)),
  "closing greater than '>'",
  _,
  _,
)

head_loc : [Token] -> Option<Loc>
head_loc(tokens) =
  if let [t | _] = tokens then
    Some(@erlang:element(2, t))
  else
    None

node_loc : Node -> Loc
node_loc(node) = @erlang:element(2, node)

token_loc : Token -> Loc
token_loc(token) = @erlang:element(2, token)

token_tag : Token -> Atom
token_tag(v) = @erlang:element(1, v)

range : (Loc, Loc) -> Loc
range(start, end) = Loc {
  start_line = start.start_line
  start_col = start.start_col
  end_line = end.end_line
  end_col = end.end_col
}

pmap : (A -> B, Parsed<A>) -> Parsed<B>
pmap(f, p) =
  let new_value = match p.value {
    Some(a) => Some(f(a))
    None => None
  }
  { p | value := new_value }

pmap_or : (A -> B, B, Parsed<A>) -> Parsed<B>
pmap_or(f, default, p) =
  let new_value = match p.value {
    Some(a) => Some(f(a))
    None => Some(default)
  }
  { p | value := new_value }

pbind : (A -> Parsed<B>, Parsed<A>) -> Parsed<B>
pbind(f, p) = match p.value {
  Some(a) =>
    let new_r = f(a)
    { new_r | errs = new_r.errs ++ p.errs }
  None => { p | value := None }
}

pbind_or : (A -> Parsed<B>, Parsed<B>, Parsed<A>) -> Parsed<B>
pbind_or(f, default, p) = match p.value {
  Some(_) => pbind(f, p)
  None => default
}

parsed : A -> Parsed<A>
parsed(a) = Parsed { value = Some(a), errs = [] }

parsed_err : (A, String, Option<Loc>) -> Parsed<A>
parsed_err(a, str, mloc) = Parsed { value = Some(a), errs = [(mloc, str)] }

add_err : (Parsed<A>, String, Option<Loc>) -> Parsed<B>
add_err(p, str, mloc) = { p | value := None, errs = [(mloc, str) | p.errs] }

one_err : (String, Option<Loc>) -> Parsed<A>
one_err(str, mloc) = Parsed { value = None, errs = [(mloc, str)] }

make_ref : () -> Ref
make_ref = @erlang:make_ref/0
